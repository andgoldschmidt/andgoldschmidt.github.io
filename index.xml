<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Andy Goldschmidt</title>
    <link>https://andgoldschmidt.github.io/</link>
      <atom:link href="https://andgoldschmidt.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Andy Goldschmidt</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2022 Andy Goldschmidt</copyright><lastBuildDate>Wed, 16 Mar 2022 15:24:00 +0000</lastBuildDate>
    <image>
      <url>https://andgoldschmidt.github.io/media/icon_hufbcf978b363d0f47c9794b839e91d566_21519_512x512_fill_lanczos_center_3.png</url>
      <title>Andy Goldschmidt</title>
      <link>https://andgoldschmidt.github.io/</link>
    </image>
    
    <item>
      <title>Invited talk, APS March Meeting</title>
      <link>https://andgoldschmidt.github.io/talks/2022_03_mm/</link>
      <pubDate>Wed, 16 Mar 2022 15:24:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/talks/2022_03_mm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Automatically differentiating numerical integrators</title>
      <link>https://andgoldschmidt.github.io/posts/jax_mpc/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/jax_mpc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;JAX&lt;/strong&gt; is a research project from Google. For our purpose, JAX is a way to do automatic differentiation: &lt;em&gt;With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives.&lt;/em&gt; Numerical integration schemes are just native python and NumPy functions so JAX can automatically differentiate them. An example of when we might want to automatically differentiating numerical integrators is the control of nonlinear dynamical systems.&lt;/p&gt;
&lt;p&gt;In particular, the purpose of this post is to learn how to do three things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use JAX.&lt;/li&gt;
&lt;li&gt;Implement a few numerical integration schemes.&lt;/li&gt;
&lt;li&gt;Use JAX to linearize a numerical integration scheme.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp
from jax import grad, jit, vmap, jacfwd, jacrev

import numpy as np
from tqdm.notebook import tqdm, trange
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
from matplotlib.cm import ScalarMappable
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;derivatives-with-jax&#34;&gt;Derivatives with JAX&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;JAX can compute derivatives through algorithms. A nice introduction is: &lt;a href=&#34;https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is a function, the rectified cube:
\begin{equation}
f(x) = |x|^3.
\end{equation}&lt;/p&gt;
&lt;p&gt;We can define $f(x)$ in a bit of a silly way by using an if statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def rectified_cube(x):
    r = 1
    if x &amp;lt; 0.:
        for i in range(3):
            r *= x
            r = -r
    else:
        for i in range(3):
            r *= x
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;JAX can differentiate this $f(x)$ no problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gradient_function = grad(rectified_cube)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1)
xs = np.linspace(-1, 1)
fx = []
d_fx = []
for x in xs:
    fx.append(rectified_cube(x))
    d_fx.append(gradient_function(x))
ax.plot(xs, fx, xs, d_fx, lw=2)
ax.legend([&#39;$|x|^3$&#39;, &#39;$\\frac{d}{dx} |x|^3$&#39;], fontsize=14, ncol=2)
ax.set_xlabel(&#39;x&#39;, fontsize=14);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_15ae8a3e28aa18ad12e25427c66ae0ea.png 400w,
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_73dd0e5af9ed8e0bac93fc01c34c1d85.png 760w,
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_15ae8a3e28aa18ad12e25427c66ae0ea.png&#34;
               width=&#34;370&#34;
               height=&#34;266&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;numerical-integration--autodiff&#34;&gt;Numerical integration + Autodiff&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Here is a continuous model:
\begin{equation}
\dot{x}(t) = f(x(t), u(t)).
\end{equation}&lt;/p&gt;
&lt;p&gt;In many situations in computing (like model predictive control), the continuous dynamics must be converted to a discrete model
\begin{equation}
x_{k + 1} = F(x_k, u_k).
\end{equation}&lt;/p&gt;
&lt;p&gt;The reason for the conversion is that controls are computed as a zero-order-hold over discrete time intervals. The full discrete list of control amplitudes can be optimized. In the continuous limit, the discrete list becomes a function. Working with functions is much harder and doesn&amp;rsquo;t allow for a scheme like MPC which depends on &lt;em&gt;taking a step&lt;/em&gt;. Plus, you can often use a simple basis to approximate function dynamics within the discrete time step (how?).&lt;/p&gt;
&lt;p&gt;The RHS term $F(x_k, u_k)$ is a numerical integration. There are many ways to do this. $F$ is frequently nonlinear. Unfortunately, we really only know how to do MPC for systems with linear discrete dynamics where
\begin{equation}
F(x_k, u_k)=A x_k + B u_k.
\end{equation}
In order to do more interesting systems, we rely on locally linear approximations of $F$ in algorithms. This means the model is local about some guess trajectory, i.e. you compute matrices like
\begin{equation}
A \equiv \nabla_x F(x, u)|_{x_g}
\end{equation}
and multiply them with $x - x_g$ (it&amp;rsquo;s just the 1st order Fourier expansion). In MPC, the choice for $x_g$ (read: x-guess) is often a recently valid solution that&amp;rsquo;s been shifted to the left to accommodate the next prediction horizon. In any case, this means we want derivatives of $F$. This is where JAX comes in: &lt;strong&gt;If we know the continuous model $f$ and the algorithm $A$ we used to compute the numerical integration (i.e. $F = A \circ f$), we can find these linear approximations with automatic differentiaton.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why might this be better? It&amp;rsquo;s hard (or at a minumum, annoying) to compute an analytic linearization of some numerical integrators $F$ even for simple nonlinear dynamics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Exercise: Are there cases where it might not be reasonable to explicitely compute derivatives?&lt;/em&gt;&lt;/p&gt;
&lt;!--- Answer: Derivatives are annying even for a simple example. For instance, try a bilinear system with Euler integration. It&#39;s still doable. Meanwhile, it is probably unreasonable to compute explicit derivatives for a neural network description of the dynamics. ---&gt;
&lt;h2 id=&#34;van-der-pol-experiment&#34;&gt;Van der Pol experiment&lt;/h2&gt;
&lt;p&gt;In this section, we&amp;rsquo;ll review some numerical integrators as preparation for thinking about how we might locally linearize them with JAX.&lt;/p&gt;
&lt;p&gt;The toy system we will use is a driven Van der Pol oscillator,
\begin{equation}
\begin{aligned}
&amp;amp;\dot{x}_1 = x_2, \&lt;br&gt;
&amp;amp;\dot{x}_2 = -x_1 + \mu (1 - x_1^2) x_2 + u
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdp(t, x, u):
        mu = 2
        x1, x2 = x
        return jnp.array([
            x2,
            -x1 + mu * (1 - x1 ** 2) * x2 + u
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;euler-integration&#34;&gt;Euler integration&lt;/h2&gt;
&lt;p&gt;The simplest choice for numerical integration is Euler integration, which combines the definition of the derivative
\begin{equation}
\dot{x}(t) = \lim_{\Delta t \rightarrow 0} \frac{\Delta x}{\Delta t}
\end{equation}
with the dynamics $\dot{x}(t) = f(t, x(t), u(t))$ such that
\begin{equation}
\frac{x_{k+1} - x_k}{\Delta t} \approx f(k, x_k, u_k)
\end{equation}
so
\begin{equation}
x_{k+1} \approx x_k + \Delta t f(k, x_k, u_k) \equiv F(x_k, u_k)
\end{equation}&lt;/p&gt;
&lt;p&gt;Set $z_k = [x_k, u_k]$ for simplicity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def euler(z, dt=1):
    return z[:2] + dt * vdp(_, z[:2], z[2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Driving is external; set a policy.
def u_fn(t):
    return jnp.zeros_like(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One fun thing to do is break the numerical integration by taking steps that are too big. Do that by dividing the interval [0,15] into 100 steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate the oscillator.
ts = jnp.linspace(0, 15, 100)
dt = ts[1] - ts[0]

t0 = ts[0]
x0 = jnp.array([[1], [-2]])

xs = [None] * (len(ts) + 1)
xs[0] = x0
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[i], u_fn(t)])
    xs[i + 1] = euler(z, dt)
xs = jnp.hstack(xs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1, x2 = xs
fig, ax = plt.subplots(1, figsize=[8,8])
ax.plot(x1, x2, lw=5)
ax.set_aspect(&#39;equal&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_e07c234137738ed42c9e41d2fa6dc3e6.png 400w,
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_ec645fb030a4e79f024a3168eaaf15ac.png 760w,
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_e07c234137738ed42c9e41d2fa6dc3e6.png&#34;
               width=&#34;275&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can compute the Jacobian of $F(x_k, u_k)$ using JAX. That is, we want to compute the derivative to arrive at a matrix, $\nabla_z F(z)$. There are a few ways to do this with JAX.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Differentiate (retain ability to use other args)
jac_euler = jacfwd(euler)

# Is prefixing values faster? Not by much.
jac_euler_2 = jacfwd(lambda z: euler(z, dt))

# Is backward faster? No way! Recall: Why not?
jac_euler_3 = jacrev(euler)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jac_xs = [None] * len(ts)
det_jac_xs = [None] * len(ts)
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[:, i][:, None], u_fn(t)])
    jac_xs[i] = jac_euler(z.squeeze(), dt)
    det_jac_xs[i] = np.linalg.det(jac_xs[i][:, :2])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$\nabla_z F(z)$ is a matrix. Our goal in a first order linearization is to compute something like $\Delta F = \nabla_z F(z) \cdot \Delta z$. Here, let&amp;rsquo;s do that for the $x$ values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the differential of F
dx = xs[:, 1:] - xs[:, :-1]
df = np.vstack([jac_xs[i][:,:2] @ dx[:, i] for i in range(len(ts))]).T
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the derivatives are only as good as our numerical integration (which we already decided to break). Keep in mind that this is the behavior you would get even if you did analytic derivatives with respect to the Euler integration scheme.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tangent vectors colored by the determinant of the Jacobian
fig, ax = plt.subplots(1, figsize=[8, 8])
cmap = plt.get_cmap(&#39;viridis&#39;)
norm = plt.Normalize(min(det_jac_xs), max(det_jac_xs))
for i in range(len(ts)):
    ax.arrow(x1[i], x2[i], df[0][i], df[1][i], width=.05,
             color=cmap(norm(det_jac_xs[i])))
ax.set_aspect(&#39;equal&#39;)
fig.colorbar(ScalarMappable(norm=norm, cmap=cmap))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_fb708b83c3764acf83b599426db472f7.png 400w,
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_56bd4bcfecec0840b15fe735c0e66bde.png 760w,
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_fb708b83c3764acf83b599426db472f7.png&#34;
               width=&#34;359&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If you want to peak at the matrix assoicated to $\nabla_z F(z)$, then you need to act on basis vectors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basis = [None] * (xs.shape[0] + 1)
for i in range(len(basis)):
    basis[i] = np.zeros([xs.shape[0] + 1, 1])
    basis[i][i] = 1
    
z0 = jnp.vstack([xs[:, 0][:, None], u_fn(t)])
np.hstack([jac_euler(z0.squeeze(), dt) @ b for b in basis])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.        , 0.15151516, 0.        ],
       [1.0606061 , 1.        , 0.15151516]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;runge-kutta-methods&#34;&gt;Runge-Kutta methods&lt;/h2&gt;
&lt;p&gt;One nice thing with JAX is that if we pick more interesting numerical integration schemes, we don&amp;rsquo;t have to worry about computing the analytic derivative of the more complicated function $F(x_k, u_k)$. Let&amp;rsquo;s see this in action.&lt;/p&gt;
&lt;p&gt;Generalize the Euler method so that
\begin{equation}
x_{k+1} \approx x_k + \Delta t \phi(k, x_k, u_k) \equiv F(x_k, u_k)
\end{equation}
with $\phi$ some new function defined to reduce errors by leveraging intermediate evaluations of $f$.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not going to derive it, but one way to improve things is the classic 4th order Runge-Kutta:&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{k+1} \approx x_k + \frac{\Delta t}{6} (f_1 + 2 f_2 + 2 f_3 + f_4)
\end{equation}
where&lt;/p&gt;
&lt;p&gt;$f_1 = f(t_k, x_k, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_2 = f(t_k + \Delta t / 2, x_k + f_1 \Delta t / 2, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_3 = f(t_k + \Delta t / 2, x_k + f_2 \Delta t / 2, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_4 = f(t_k + \Delta t, x_k +  f_3 \Delta t, u_k)$.&lt;/p&gt;
&lt;p&gt;Why do we modify the $x$ arguments in the function, but not the $u$ arguments? First off, we assume $f$ is a known function, but we don&amp;rsquo;t know anything about the control policy. Our control assumption is actually zero-order-hold so we are assuming $u_k$ won&amp;rsquo;t vary within our step. This is pretty important for the validity of the scheme. Luckily we&amp;rsquo;re in charge of the control so we can set this.&lt;/p&gt;
&lt;p&gt;For completeness, let&amp;rsquo;s just say we don&amp;rsquo;t make the assumption of zero order hold. Then the Runge-Kutta approximation is not going to satisfy the promised accuracy, and we don&amp;rsquo;t have any way to improve the integration by cancelling Fourier terms like we did with $x$ (this is because $u$ is not constrained by a function). One final way to think about this: if we did know the policy, then $f(t, x, u) \mapsto f(t, x, \pi(t, x)) \equiv f^\pi(t, x)$ and we can have no gradients with respect to $u$ because it is fixed by the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Again, we don&#39;t have explicit time dependence. 
# Exercise: How would things change?
def rk4(z, dt=1):
    f1 = vdp(_, z[:2], z[2])
    f2 = vdp(_, z[:2] + f1 * dt / 2, z[2])
    f3 = vdp(_, z[:2] + f2 * dt / 2, z[2])
    f4 = vdp(_, z[:2] + f3 * dt, z[2])
    return z[:2] + (f1 + 2 * f2 + 2 * f3 + f4) * dt / 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pick the same time scheme as before, but now we&amp;rsquo;re going to get a much better numerical integration with our improved method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate the oscillator.
xs = [None] * (len(ts) + 1)
xs[0] = x0
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[i], u_fn(t)])
    xs[i + 1] = rk4(z, dt)
xs = jnp.hstack(xs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1, x2 = xs
fig, ax = plt.subplots(1, figsize=[8,8])
ax.plot(x1, x2, lw=5)
ax.set_aspect(&#39;equal&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_ff99742bcbe18cf709e955ffc73a47f4.png 400w,
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_32f801381eaeff81727c764fd973909e.png 760w,
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_ff99742bcbe18cf709e955ffc73a47f4.png&#34;
               width=&#34;247&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Repeat the same differentiation process using JAX to get improved derivatives inherited directly from the improved numerical integration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Differentiate
jac_rk4 = jacfwd(rk4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jac_xs = [None] * len(ts)
det_jac_xs = [None] * len(ts)
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[:, i][:, None], u_fn(t)])
    jac_xs[i] = jac_rk4(z.squeeze(), dt)
    det_jac_xs[i] = np.linalg.det(jac_xs[i][:, :2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  0%|          | 0/100 [00:00&amp;lt;?, ?it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the differential of F
dx = xs[:, 1:] - xs[:, :-1]
df = np.vstack([jac_xs[i][:,:2] @ dx[:, i] for i in range(len(ts))]).T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tangent vectors colored by the determinant of the Jacobian
fig, ax = plt.subplots(1, figsize=[8, 8])
cmap = plt.get_cmap(&#39;viridis&#39;)
norm = plt.Normalize(min(det_jac_xs), max(det_jac_xs))
for i in range(len(ts)):
    ax.arrow(x1[i], x2[i], df[0][i], df[1][i], width=.05,
             color=cmap(norm(det_jac_xs[i])))
ax.set_aspect(&#39;equal&#39;)
fig.colorbar(ScalarMappable(norm=norm, cmap=cmap))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_4c630a5bdc5bf5a8dbc06632e4f234b5.png 400w,
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_c398f8c18dba9a2ba70ca381359fe9ed.png 760w,
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_4c630a5bdc5bf5a8dbc06632e4f234b5.png&#34;
               width=&#34;318&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Nice derivatives.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://andgoldschmidt.github.io/pages/spectral_help/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/pages/spectral_help/</guid>
      <description>&lt;h2 id=&#34;helper-functions-for-spectral-dmd&#34;&gt;Helper functions for spectral DMD.&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python:&#34;&gt;    # MIT License

    # Copyright (c) 2022 Andy Goldschmidt

    # Permission is hereby granted, free of charge, to any person obtaining a copy
    # of this software and associated documentation files (the &amp;quot;Software&amp;quot;), to deal
    # in the Software without restriction, including without limitation the rights
    # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    # copies of the Software, and to permit persons to whom the Software is
    # furnished to do so, subject to the following conditions:

    # The above copyright notice and this permission notice shall be included in all
    # copies or substantial portions of the Software.

    # THE SOFTWARE IS PROVIDED &amp;quot;AS IS&amp;quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    # SOFTWARE.

    import numpy as np

    def BigOmg(omega, ts):
        &#39;&#39;&#39;
        Construct time-domain coordinates for Spectral DMD.
        
        Args:
            omega (`1d-array`): 1d array of frequencies 
            ts (`1d-arary`): 1d array of times
            
        Returns:
            `nd-array`: 2*len(omega) by len(t) array of $[\cos(\vec{omega} t), \sin(\vec{omega} t)]^T$
        &#39;&#39;&#39;
        omt = omega.reshape(-1,1)@ts.reshape(1,-1)
        return np.vstack([np.cos(2*np.pi*omt), np.sin(2*np.pi*omt)])


    def loss(X, A, omega, ts):
        &#39;&#39;&#39;
        Loss function for Spectral DMD.
        
        Returns:
            float: Evaluation of loss function.
        &#39;&#39;&#39;
        return np.linalg.norm(X - A@BigOmg(omega,ts))


    def grad_loss(X, A, omega, ts):
        &#39;&#39;&#39;
        Gradient of the loss function w.r.t. $\omega$.
        &#39;&#39;&#39;
        n_omg = len(omega)
        part2 = -4*np.pi*A.T@(X - A@BigOmg(omega,ts))
        grad_res = [0]*n_omg
        for i in range(n_omg):
            grad_res[i] += (-np.sin(2*np.pi*omega[i]*ts)*ts).dot(part2[i,:])
            grad_res[i] += (np.cos(2*np.pi*omega[i]*ts)*ts).dot(part2[n_omg + i,:])
        return np.array(grad_res).reshape(1,-1)


    def grad_loss_j(j, X, A, omega, ts):
        &#39;&#39;&#39;
        Gradient of the loss function w.r.t. $\omega_j$
        
        Returns:
            float: Gradient of the loss w.r.t $\omega_j$
        &#39;&#39;&#39;
        n_omg = len(omega)
        if j &amp;gt; n_omg - 1:
            raise ValueError(&amp;quot;Invalid value. Index j={} exceeds len(omega)={}.&amp;quot;.format(j,n_omg))
            
        part2 = -4*np.pi*A.T@(X - A@BigOmg(omega,ts))
        grad_res_j = (-np.sin(2*np.pi*omega[j]*ts)*ts).dot(part2[j,:])
        grad_res_j += (np.cos(2*np.pi*omega[j]*ts)*ts).dot(part2[n_omg + j,:])
        return grad_res_j


    def residual_j(j, X, A, omega, ts):
        &#39;&#39;&#39;
        Residual for the data trajectory X using the model A, $\Omega$. The residual excludes the contribution
        of the frequency $\omega_j$.
        
        TODO:
        * Check the case where A.shape[0] = 1
        
        Returns:
            `ndarray`: residual of shape X.shape[0] by ts.shape[0]
        &#39;&#39;&#39;
        n_omega = len(omega)
        if j &amp;gt; n_omega - 1:
            raise ValueError(&amp;quot;Invalid value. Index j={} exceeds len(omega)={}.&amp;quot;.format(j, n_omega))
        
        j2 = j + n_omega
        indices = np.hstack([np.arange(j), np.arange(j+1,j2), np.arange(j2+1, 2*n_omega)])
        return X - A[:,indices]@(BigOmg(omega,ts)[indices, :])


    def update_A(X, omega, ts, threshold, threshold_type):
        # Update A
        # -- This step could get some DMD love
        U,S,Vt = np.linalg.svd(BigOmg(omega, ts), full_matrices=False)
        if threshold_type == &#39;count&#39;:
            r = threshold
        elif threshold_type == &#39;percent&#39;:
            r = np.sum(S/np.max(S) &amp;gt; threshold)
        rU = U[:,:r]
        rS = S[:r]
        rVt = Vt[:r, :]
        return X@np.conj(rVt.T)@np.diag(1/rS)@np.conj(rU.T)


    def max_fft_update(result, dt):
            # Real signal means the other half of the hat are complex conjugates
            n = result.shape[1]
            n_sym = n//2 if n % 2 == 0 else n//2 + 1
            
            # Compute fft
            res_hat = np.fft.fft(result, axis=1)
            
            # Get the maximum freq. coordinate considering all data dimensions
            ires = np.argmax(np.sum(np.abs(res_hat[:,:n_sym]), axis=0))
            res_freq = np.fft.fftfreq(n, dt)[:n_sym]
            return res_freq[ires]
            

    # Accelerated proximal gradient descent
    # -----------------------------------------------------------------------------
    def optimizeWithAPGD(x0, func_f, func_g, grad_f, prox_g, beta_f, tol=1e-6, max_iter=1000, verbose=False):
        &amp;quot;&amp;quot;&amp;quot;
        Optimize with Accelerated Proximal Gradient Descent Method
            min_x f(x) + g(x)
        where f is beta smooth and g is proxiable.
        
        input
        -----
        x0 : array_like
            Starting point for the solver
        func_f : function
            Input x and return the function value of f
        func_g : function
            Input x and return the function value of g
        grad_f : function
            Input x and return the gradient of f
        prox_g : function
            Input x and a constant float number and return the prox solution
        beta_f : float
            beta smoothness constant for f
        tol : float, optional
            Gradient tolerance for terminating the solver.
        max_iter : int, optional
            Maximum number of iteration for terminating the solver.
            
        output
        ------
        x : array_like
            Final solution
        obj_his : array_like
            Objective function value convergence history
        err_his : array_like
            Norm of gradient convergence history
        exit_flag : int
            0, norm of gradient below `tol`
            1, exceed maximum number of iteration
            2, others
        &amp;quot;&amp;quot;&amp;quot;
        # initial information
        x = x0.copy()
        y = x0.copy()
        g = grad_f(y)
        t = 1.0
        #
        step_size = 1.0/beta_f
        # not recording the initial point since we do not have measure of the optimality
        obj_his = np.zeros(max_iter)
        err_his = np.zeros(max_iter)
        
        # start iteration
        iter_count = 0
        err = tol + 1.0
        while err &amp;gt;= tol:
            #####
            # Accelerated proximal gradient
            x_new = prox_g(y - step_size*g, step_size)
            t_new = (iter_count - 1)/(iter_count + 2)
            y_new = x_new + t_new*(x_new - x)
            # FISTA version:
            # t_new = (1 + np.sqrt(1+4*t**2))/2
            # y_new = x_new + (t - 1)/t_new*(x_new - x)
            #####
            #
            # update information
            obj = func_f(x_new) + func_g(x_new)
            err = np.linalg.norm(x - x_new)
            #
            np.copyto(x, x_new)
            np.copyto(y, y_new)
            t = t_new
            g = grad_f(y)
            #
            obj_his[iter_count] = obj
            err_his[iter_count] = err
            #
            # check if exceed maximum number of iteration
            iter_count += 1
            if iter_count &amp;gt;= max_iter:
                if verbose:
                    print(&#39;Proximal gradient descent reach maximum of iteration&#39;)
                return x, obj_his[:iter_count], err_his[:iter_count], 1
        #
        return x, obj_his[:iter_count], err_his[:iter_count], 0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Invited talk, MMLDT-CSET</title>
      <link>https://andgoldschmidt.github.io/talks/2021_09_mmldtcset/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/talks/2021_09_mmldtcset/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bilinear dynamic mode decomposition for quantum control</title>
      <link>https://andgoldschmidt.github.io/publication/bidmd/</link>
      <pubDate>Mon, 22 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/publication/bidmd/</guid>
      <description>&lt;!-- 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;!-- &lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;
 --&gt;
&lt;p&gt;&lt;strong&gt;Caption for the figure:&lt;/strong&gt; The trajectory of a qubit driven by a linearly-polarized semi-classical drive $\textrm{u}(t)$ (Hamiltonian: $H(t) = \pi \sigma_z + \textrm{u}(t) \sigma_x$) is shown on the Bloch sphere in (a). The corresponding Pauli-spin measurements are shown in (b). Measurements $\mathbf{x}_j$, $j=1,2,\dots$, are taken at discrete time steps and assembled into offset snapshot matrices $\mathbf{X}$ and $\mathbf{X}&#39;$ in (c). The bilinear Dynamic Mode Decomposition (d) is a regression-based algorithm that uses the assembled data matrices and control input from sufficiently-resolved data to learn the intrinsic dynamics, $\mathbf{A}$ and the control, $\mathbf{B}$, for the bilinear dynamics.&lt;/p&gt;
&lt;!-- Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Minisymposium organizer, SIAM CSE</title>
      <link>https://andgoldschmidt.github.io/talks/2021_03_siamcse/</link>
      <pubDate>Mon, 01 Mar 2021 12:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/talks/2021_03_siamcse/</guid>
      <description>&lt;h2 id=&#34;session-1&#34;&gt;Session 1&lt;/h2&gt;
&lt;body&gt;
&lt;dl&gt;
	&lt;dt&gt;
	&lt;strong&gt;
	9:45-10:00 Calibrating Quantum Hardware with Online Optimal Control&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109256&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Jonathan L. Dubois&lt;/em&gt;, Lawrence Livermore National Laboratory, U.S. 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	10:05-10:20 Bilinear Dynamic Mode Decomposition for Quantum Control&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109258&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Andy Goldschmidt&lt;/em&gt; and
								Eurika Kaiser, University of Washington, U.S.; Jonathan L. Dubois, Lawrence Livermore National Laboratory, U.S.; Steve Brunton, University of Washington, U.S.; J. Nathan Kutz, University of Washington, Seattle, U.S. 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	10:25-10:40 Designing High-Fidelity Controls on Real Quantum Systems using&amp;nbsp;System&amp;nbsp;Identification and Reinforcement Learning&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109264&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Michael J. Biercuk&lt;/em&gt;, University of Sydney, Australia; Harrison Ball, Q-CTRL, Australia; Yuval Baum, California Institute of Technology, U.S.; Andre Carvalho, Griffith University, Brisbane, Australia; Leonardo de Castro,
								Sean Howell,
								Michael Hush, and
								Per Liebermann, Q-CTRL, Australia; Pranav Mundada, Princeton University, U.S.; Felix Thomsen, Q-CTRL, Australia 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	10:45-11:00 Quantum System Compression: A Hamiltonian Guided Walk through Hilbert Space&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109702&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Robert L. Kosut&lt;/em&gt;, SC Solutions, U.S.; Herschel Rabitz and
								Tak-San Ho, Princeton University, U.S. 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	11:05-11:20 Learning the States of Quantum Dot Systems: A Ray-Based Framework&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109703&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Justyna Zwolak&lt;/em&gt;, National Institute of Standards and Technology, U.S. 
	&lt;/dd&gt;&lt;/dl&gt;
&lt;/body&gt;
&lt;h2 id=&#34;session-2&#34;&gt;Session 2&lt;/h2&gt;
&lt;body&gt;
&lt;dl&gt;
	&lt;dt&gt;
	&lt;strong&gt;
	2:15-2:30 A General Theory of Randomized Benchmarking&lt;/strong&gt;   
    &lt;a href=&#34;dsp_talk.cfm?p=109705&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Jens Eisert&lt;/em&gt;, Freie UniversitÃ¤t Berlin, Germany; Jonas Helsen, University of Amsterdam, Netherlands; Ingo Roth, Freie UniversitÃ¤t Berlin, Germany; Emilio Onorati, University College London, United Kingdom; Albert H. Werner, University of Copenhagen, Denmark 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	2:35-2:50 Operational, Gauge-Free Quantum Tomography&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109706&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Olivia Di Matteo&lt;/em&gt;, TRIUMF, Canada; John Gamble and
								Chris Granade, Microsoft Research, U.S.; Kenneth Rudinger, Sandia National Laboratories, U.S.; Nathan O. Wiebe, University of Washington, U.S. 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	2:55-3:10 Linear Preservers on Infinitely Divisible Matrices through Separability&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109941&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Indu L&lt;/em&gt; and
								Jill K Mathew, Mar Ivanios College, Trivandrum, Kerala, India 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	3:15-3:30 Machine Learning for Quantum States&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109708&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; &lt;em&gt; Giuseppe Carleo&lt;/em&gt;, EPFL, Switzerland 
	&lt;/dd&gt;&lt;dt&gt;
	&lt;strong&gt;
	3:35-3:50 Learning Models of Quantum Systems from Experiments&lt;/strong&gt; 
    &lt;a href=&#34;dsp_talk.cfm?p=109710&#34;&gt;abstract&lt;/a&gt;
	&lt;/dt&gt;&lt;dd&gt; Antonio Gentile and
								Brian Flynn, University of Bristol, United Kingdom; Sebastian Knauer, University of Vienna, Austria; &lt;em&gt; Nathan O. Wiebe&lt;/em&gt;, University of Washington, U.S.; Stefano Paesani, University of Bristol, United Kingdom; Chris Granade, Microsoft Research, U.S.; John Rarity,
								Raffaele Santagati, and
								Anthony Laing, University of Bristol, United Kingdom 
	&lt;/dd&gt;&lt;/dl&gt;
&lt;/body&gt;
</description>
    </item>
    
    <item>
      <title>Spectral dynamic mode decomposition</title>
      <link>https://andgoldschmidt.github.io/posts/spectral_dmd/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/spectral_dmd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Spectral dynamic mode decomposition&lt;/strong&gt; is my term for Algorithm 1 from the paper &lt;em&gt;From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction&lt;/em&gt; by H. Lange, S.L. Brunton, J.N. Kutz (&lt;a href=&#34;https://www.youtube.com/watch?v=RBYFsFr4soo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;) (&lt;a href=&#34;https://arxiv.org/abs/2004.00574&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;). Necessary background is a familiarity with the dynamic mode decomposition (&lt;a href=&#34;https://www.youtube.com/watch?v=sQvrK8AGCAo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;) (&lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_mode_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia entry&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This project walks through a toy example I coded up in Python. The example is similar to one from the paper which gets across the main points. I have packed away the main functions for the algorithm in the utility &lt;code&gt;spectral_help.py&lt;/code&gt; which has been linked in this project&amp;rsquo;s description.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from numpy.linalg import norm, solve

import matplotlib.pyplot as plt
cmap = plt.get_cmap(&#39;tab20&#39;)

from spectral_help import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-definition&#34;&gt;Example Definition&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class toy_data:
    &amp;quot;&amp;quot;&amp;quot; 
    Generate toy oscillation data at specific frequencies.
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, tf, npts, noise):
        &amp;quot;&amp;quot;&amp;quot;
        Parameters:
            tf: final time
            npts: number of samples between 0 and tf
            noise: standard deviation of additive Gaussian noise
        &amp;quot;&amp;quot;&amp;quot;
        self.tf = tf
        self.npts = npts
        self.ts = np.linspace(0, self.tf, self.npts)
        self.dt = self.ts[1] - self.ts[0]
        self.noise = noise
        
        # Manual toy data (a stack of sines)
        self.freqs = [0.5, 2, 0.75, 3]
        self.X_fn = lambda ts: np.vstack([(np.sin([2 * np.pi * self.freqs[0] * ts]) + np.sin([2 * np.pi * self.freqs[1] * ts])),
                                          (np.sin([2 * np.pi * self.freqs[2] * ts]) + np.sin([2 * np.pi * self.freqs[3] * ts]))])
        
        # Normalize, add noise
        X = self.X_fn(self.ts)
        self.X_mean = np.mean(X, axis=1).reshape(-1,1)
        self.X_std = np.std(X, axis=1).reshape(-1,1)
        X = (X - self.X_mean)/self.X_std
        self.X_true = np.copy(X)
        self.X = X + np.random.randn(*X.shape)*self.noise
        
        # Construct test vars
        self.ts_test = None
        self.X_test = None
        
        
    def run_test(self, tf_predict, npts_predict):
        &amp;quot;&amp;quot;&amp;quot;
        Parameters:
            tf_predict: final time
            npts_predict: number of points between 0 and tf_predict
            
        Updates:
            self.ts_test: test time series
            self.X_test_true: normalized ground truth for the test
        &amp;quot;&amp;quot;&amp;quot;
        self.ts_test = np.linspace(0, tf_predict, npts_predict)
        X_test = self.X_fn(self.ts_test)
        
        # Use training std_dev and mean
        self.X_test_true = (X_test - self.X_mean)/self.X_std 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;run-the-experiment&#34;&gt;Run the Experiment&lt;/h1&gt;
&lt;p&gt;We run the spectral dynamic mode decomposition algorithm to learn the frequencies of an operator generating our toy time-series data. Because of the nature of the algorithm, we can do this with very noisy data (Here, we set the standard deviation at 0.5 for mean-zero variance-one toy training data).&lt;/p&gt;
&lt;p&gt;First, we configure the model. Then we set up the algorithm and run the optimization. The optimization is performed using the Accelerated Proximal Gradient Descent Method, or AGPD. The reason is because we are imposing sparsity using a $\ell_1$ norm&amp;ndash;that is, a LASSO-type optimization.&lt;/p&gt;
&lt;h2 id=&#34;1-configure-the-model&#34;&gt;1. Configure the model&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Config
# ======
np.random.seed(1)

# Data params
npts = 400             # number of time points
tf = 8                 # max time
std_noise = .5         # set the amount of noise on the mean-0, variance-1 data
predict_factor = 3     # prediction goes out to factor*T
exper = toy_data(tf, npts, std_noise)

# Algorithm params
freq_dim = 24          # freq. for algo to try
learning_rate = 1e-3   # LR = 1/beta from beta-smooth obj. bound (at least ideally--I&#39;m just choosing a number here)
reg_factor = 5         # regularization on sparsity

# SVD parameters 
threshold_type = &#39;percent&#39; # choose &#39;count&#39; for number or &#39;percent&#39; for s/max(s) &amp;gt; threshold
threshold = 1e-1

# Plot toggle
print_omega_updates = True
def print_update(omg, title):
    if print_omega_updates:
        print(&#39;{} $\omega$:\t&#39;.format(title), np.sort(np.round(omg[omg.astype(bool)], 3)))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;2-set-up-the-algorithm--3-optimize&#34;&gt;2. Set up the algorithm &amp;amp; 3. Optimize&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Algorithm
# =========
# 1. Initialize
omega = np.zeros(freq_dim)*2
print_update(omega, &#39;Initial&#39;)
A = np.random.rand(exper.X.shape[0], freq_dim*2)

obj_his = []
err_his = []

# 2. FFT to obtain the initial starting point for the optimization
for ifreq in range(len(omega)):
    # - Construct the residual via the current frequencies
    res = residual_j(ifreq, exper.X, A, omega, exper.ts)

    # - Select the maximum fft frequency as the initial value
    omega[ifreq] = max_fft_update(res, exper.dt)

    # - Update A
    A = update_A(exper.X, omega, exper.ts, threshold, threshold_type)

# 3. Perform proximal gradient descent from the initial point
# - Construct optimization functions
lam_cs = reg_factor*norm(A.T.dot(exper.X), np.inf)
def f(w):
    return loss(exper.X, A, w.flatten(), exper.ts)
def gradf(w):  
    return grad_loss(exper.X, A, w.flatten(), exper.ts)
def func_g(w):
    return lam_cs*np.linalg.norm(w, ord=1)
def prox_g(w, t):
    res = []
    r = t*lam_cs
    for wi in w.flatten():
        if wi &amp;gt; r:
            res.append(wi-r)
        elif wi &amp;lt; -r:
            res.append(wi+r)
        else:
            res.append(0)
    return np.array(res)

# - Optimization algorithm
w, iobj_his, ierr_his, cond = optimizeWithAPGD(omega.reshape(1,-1), f, func_g, gradf, prox_g, (1/learning_rate)*npts, max_iter=5000, verbose=True)
obj_his.append(iobj_his)
err_his.append(ierr_his)
omega = w
print_update(omega, &#39;Final  &#39;)

# 4. Final operator update
A = update_A(exper.X, omega, exper.ts, threshold, threshold_type)
print_update(np.array(exper.freqs), &#39;Expected&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Initial $\omega$:	 []
Final   $\omega$:	 [0.494 0.743 1.991 2.99 ]
Expected $\omega$:	 [0.5  0.75 2.   3.  ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We printed out the frequencies (previous cell). In the next cell, we show the objective value and the gradient of the objective for the iterations of the optimization. The characteristic oscillations of an accelerated gradient descent are observed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot 
# ====
# Inspect convergence results
fig,axes = plt.subplots(2,1,figsize=[10,10])
ax = axes[0]
ax.plot(iobj_his)
ax.set_ylabel(&#39;Obj. value&#39;)
ax.set_xlabel(&#39;Iterations&#39;)
ax.set_yscale(&#39;log&#39;)
ax = axes[1]
ax.plot(ierr_his)
ax.set_ylabel(&#39;Gradient magn.&#39;)
ax.set_xlabel(&#39;Iterations&#39;)
ax.set_yscale(&#39;log&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_da992a4cde976d6430ac9d6838d40a67.png 400w,
               /posts/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_fa9ef638480d69f3b09d71ebdc3dc0df.png 760w,
               /posts/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_da992a4cde976d6430ac9d6838d40a67.png&#34;
               width=&#34;635&#34;
               height=&#34;601&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;We see the training (top) and test (bottom) results for our two-dimensional multi-frequency dynamics.&lt;/p&gt;
&lt;p&gt;Notice the order of magnitude increase in the horizontal time axis on the bottom plot. On this plot, we observe that the solutions match the test simulations and are stable because of the model assumptions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make prediction
exper.run_test(tf*predict_factor, 10*npts) # keep sample freq the same

bigOmg_test = BigOmg(omega, exper.ts_test)
X_pred = A@(bigOmg_test)


# Plot data
fig,axes = plt.subplots(1,2,figsize=[20,3])
fig.subplots_adjust(hspace=0.3, wspace=0.2)
leg_params = {&#39;loc&#39;: &#39;upper right&#39;, &#39;shadow&#39;: True, &#39;fancybox&#39;: True}

ax = axes[0]
ax.plot(exper.ts, exper.X_true[0], color=cmap(1), label=&#39;Simulation&#39;)
ax.plot(exper.ts, exper.X[0], ls=&#39;&#39;, marker=&#39;+&#39;, color=cmap(0), label=&#39;Training Data&#39;)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;x&#39;)
ax.legend(**leg_params)
ax = axes[1]
ax.plot(exper.ts, exper.X_true[1], color=cmap(3), label=&#39;Simulation&#39;)
ax.plot(exper.ts, exper.X[1], ls=&#39;&#39;, marker=&#39;+&#39;, color=cmap(2), label=&#39;Training Data&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;y&#39;)
ax.set_ylim([-3.5,3.5])

# Plot model
fig,axes = plt.subplots(2,1,figsize=[20,5])
fig.subplots_adjust(hspace=0.3, wspace=0.2)
leg_params = {&#39;loc&#39;: &#39;upper right&#39;, &#39;shadow&#39;: True, &#39;fancybox&#39;: True}

ax = axes[0]
ax.plot(exper.ts_test, exper.X_test_true[0], color=cmap(1), label=&#39;Test Simulation&#39;)
ax.plot(exper.ts_test, X_pred[0], ls=&#39;-&#39;, color=cmap(0), label=&#39;Model Prediction&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;x&#39;)
ax = axes[1]
ax.plot(exper.ts_test, exper.X_test_true[1], color=cmap(3), label=&#39;Test Simulation&#39;)
ax.plot(exper.ts_test, X_pred[1], ls=&#39;-&#39;, color=cmap(2), label=&#39;Model Prediction&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;y&#39;)
ax.set_ylim([-3.5,3.5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Click on the figures to zoom.)&lt;/p&gt;
&lt;p&gt;Training data:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_0aaee56126f71e3c3b2081ee028f035b.png 400w,
               /posts/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_40f177da89af1e202d5e880b3bcb392e.png 760w,
               /posts/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_0aaee56126f71e3c3b2081ee028f035b.png&#34;
               width=&#34;760&#34;
               height=&#34;142&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Test data:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_e7a65ff9c7a4bf3ef0623c0f8f182904.png 400w,
               /posts/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_bec5f35f2817b20d7d9e4c24bafd0ffa.png 760w,
               /posts/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_e7a65ff9c7a4bf3ef0623c0f8f182904.png&#34;
               width=&#34;760&#34;
               height=&#34;212&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Better numerical derivatives for data</title>
      <link>https://andgoldschmidt.github.io/posts/derivative/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/derivative/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt; is an open-source project I started in 2019-2020 that turned into a collaboration with Markus Quade (Github, &lt;a href=&#34;https://github.com/Ohjeah&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Ohjeah&lt;/a&gt;) and Brian de Silva (Github, &lt;a href=&#34;https://github.com/briandesilva&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@briandesilva&lt;/a&gt;). It is a standalone suite of numerical differentiation methods for noisy time series data written in Python.&lt;/p&gt;
&lt;p&gt;The goal is to provide some common numerical differentiation techniques that showcase improvements that can be made on finite differences when data is noisy. The package binds these common differentiation methods to a single easily implemented differentiation interface to encourage user adaptation.&lt;/p&gt;
&lt;p&gt;Derivative is a contribution to &lt;a href=&#34;https://github.com/dynamicslab/pysindy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySINDy&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/186055899&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/186055899.svg&#34; style=&#34;width: 200px;&#34; alt=&#34;DOI&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;td&gt; &lt;a href=&#34;https://pysindy.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pysindy/badge/?version=latest&#34; style=&#34;width: 100px;&#34; alt=&#34;Documentation Status&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;td&gt; &lt;a href=&#34;https://github.com/dynamicslab/pysindy/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dynamicslab/pysindy.svg?style=social&amp;label=Star&amp;maxAge=2592000&#34; style=&#34;width: 100px;&#34; alt=&#34;GitHub stars&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;PySINDy is an open source Python package for the Sparse Identification of Nonlinear Dynamical systems (SINDy).&lt;/p&gt;
&lt;p&gt;At some point, I&amp;rsquo;ll write a post about my version of total variational regularization (see the figure above). I adapted a technique from &lt;em&gt;The solution path of the generalized lasso&lt;/em&gt; (&lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-39/issue-3/The-solution-path-of-the-generalized-lasso/10.1214/11-AOS878.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.1214/11-AOS878&lt;/a&gt;) by R.J. Tibshirani &amp;amp; J. Taylor to write a nice variation of the classic algorithm in &lt;em&gt;Numerical Differentiation of Noisy, Nonsmooth Data&lt;/em&gt; (&lt;a href=&#34;https://doi.org/10.5402/2011/164564&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.5402/2011/164564&lt;/a&gt;) by Rick Chartrand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical clustering with prototypes</title>
      <link>https://andgoldschmidt.github.io/posts/pyprotoclust/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/pyprotoclust/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Pyprotoclust&lt;/strong&gt; is an implementatin of representative hierarchical clustering using minimax linkage. The original algorithm is from &lt;em&gt;Hierarchical Clustering With Prototypes via Minimax Linkage&lt;/em&gt; (&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4527350/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.1198/jasa.2011.tm10183&lt;/a&gt;) by J. Bien and R. Tibshirani; Pyprotoclust takes a distance matrix as input. It returns a linkage matrix encoding the hierachical clustering as well as an additional list labelling the prototypes associated with each clustering.&lt;/p&gt;
&lt;p&gt;I coded up a fun example inspired by the original paper where I apply the algorithm to determine representative pictures for the Olivetti Faces dataset. It can be found &lt;a href=&#34;https://pyprotoclust.readthedocs.io/en/latest/notebooks/Example.html#Olivetti-Faces&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the Pyprotoclust documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure&lt;/strong&gt;: (Left) A dendrogram of the hierarchical clustering example with a dashed line at the example cut height. (Right) A scatter plot of the example with circles centered at prototypes drawn with radii equal to the top-level linkage heights of each cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python package development</title>
      <link>https://andgoldschmidt.github.io/posts/poetry/</link>
      <pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/poetry/</guid>
      <description>&lt;p&gt;There are lots of features that go into packaging up Python code for other users. How can we make a project that ships easily to users and takes advantage of our normal development tools? We&amp;rsquo;ll discuss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poetry: for easily making and publishing a package&lt;/li&gt;
&lt;li&gt;Sphinx: for making documentation&lt;/li&gt;
&lt;li&gt;Readthedocs: free professional-looking documenation hosting and formatting&lt;/li&gt;
&lt;li&gt;PyCharm: the default Python IDE (you can get the professional version as a student)&lt;/li&gt;
&lt;li&gt;PyPI: Python pacakge index, where you store stuff that people can pip install.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;p&gt;Read the &lt;a href=&#34;https://python-poetry.org/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Poetry docs&lt;/a&gt; to install. It&amp;rsquo;s good documentation; you should skim the Installation and Basic Usage first. For zsh users, make sure Poetry got added to your Path inside zshrc. For bash people, it&amp;rsquo;s automatic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a new project with Poetry.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a python project. (See &lt;em&gt;Choosing a project name&lt;/em&gt; below this list.)
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry new myproject
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Change to this directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Start the poetry virtual environment.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry shell
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will make a virtual environment that is like a fresh Python installation for us to be explicit about our package&amp;rsquo;s dependencies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Install new pacakges as needed.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry add python_package
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The name python_pacakge would be something like numpy. Poetry will install the package to the virtual environment and add the package to the pyproject.toml file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Take a look at the pyproject.toml file. All your package settings are here. Any added pacakges appear automatically. We also have a file not for human consumption called poetry.lock which does all the work of building the exact environement we are using. It can be good to commit this poetry.lock to version control so someone interacting with our package knows the exact packages we were using.&lt;/li&gt;
&lt;li&gt;To update all pacakges added with &lt;em&gt;poetry add&lt;/em&gt;, run &lt;em&gt;poetry update&lt;/em&gt;. To install the current system and update the poetry.lock file, run &lt;em&gt;poetry install&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;choosing-a-project-name&#34;&gt;Choosing a project name&lt;/h3&gt;
&lt;p&gt;A new poetry project ``myproject&#39;&#39; has a specific directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    myproject
    |-- pyproject.toml
    |-- README.rst
    |-- myproject
    |   |-- __init__.py
    |-- tests
        |-- __init__.py
        |-- test_myproject.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The project name is for the top level directory of the project. This is the name of the github repository and the PyPI project so it is invoked with &lt;code&gt;pip install myprojectname&lt;/code&gt;. It should be unique. Project names on PyPI should NOT use dashes (&lt;a href=&#34;https://stackoverflow.com/questions/8350853/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stackoverflow.com/questions/8350853/)&lt;/a&gt;. Underscores are allowed but discouraged. The package or module name is the inner directory containing __init__.py. This is the code that will be invoked by the user as &lt;em&gt;import my_project_name&lt;/em&gt; in code. It does not have to be unique. It can use underscores.Note that Poetry defaults to matching project and package names. This is also the Python style guideline (PEP 423).&lt;/p&gt;
&lt;h3 id=&#34;version-control&#34;&gt;Version control&lt;/h3&gt;
&lt;p&gt;Now we start tracking our new package on version control. Init a git repo in the project directory. Do this in your usual way. (e.g. hosting on github). The splash page for your package will be README.rst! Make it pretty.&lt;/p&gt;
&lt;h3 id=&#34;pycharm&#34;&gt;PyCharm&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a modern IDE. Open up the project in PyCharm.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Get the location of the interpreter for this virtual environment for PyCharm. Run this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;poetry run which python
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make the virtual environment default for PyCharm.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Settings $\rightarrow$ Project $\rightarrow$ Python Interpreter&lt;/li&gt;
&lt;li&gt;Click the gear, and select add.&lt;/li&gt;
&lt;li&gt;Choose the option \textit{existing environment} and add the path to the poetry virtual environment. Apply changes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now PyCharm will complain when you try to use python code you haven&amp;rsquo;t added. PyCharm will also give you actions to import missing libraries. However, be sure to add the python libraries with poetry, not PyCharm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;add-documentation-with-sphinx&#34;&gt;Add documentation with Sphinx&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s set up Sphinx.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Start&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir docs
poetry add sphinx
cd docs
sphinx-quickstart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command line will prompt you with a few questions. Use the default settings, but enter any project-specific information as needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All Sphinx settings are in conf.py. The first setting to edit is the path. Uncomment the lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import sys
sys.path.insert(0, os.path.abspath(&#39;.&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and change the &amp;ldquo;.&amp;rdquo; to &amp;ldquo;..&amp;rdquo; to reflect the docs folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure that Sphinx knows that the main file is index.rst by adding the lines&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Assign the master document
master_doc = &#39;index&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to conf.py.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test to see that your docs compile. Run the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;inside the docs folder then open up index.html in your web browser.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add docs to readthedocs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://readthedocs.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;readthedocs&lt;/a&gt;, login, then find and click import.&lt;/li&gt;
&lt;li&gt;Paste the link to the github repo and create.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Readthedocs will find the conf.py file and build the documentation.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Check that github will let readthedocs know when the documentation is updated. Go to the project repo settings and confirm that the Webhooks tab includes readthedocs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Note:
Oddly, the default Poetry config section tool.poetry.dependencies that allows users of your package to avoid installing development tools like Sphinx is not supported by readthedocs (the listed packages won&amp;rsquo;t be used). There is an alternative, e.g.&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sphinx = {version=&amp;quot;^3.0.2&amp;quot;, optional = true}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;To add packages to Poetry as optional you can call poetry add sphinx &amp;ndash;optional to autofill this format.
In your .readthedocs.yaml file, you can make sure these packages are installed by adding the extra_name parameter to extra_requirements, e.g.:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python:
    version: 3.7
    install:
    - method: pip
        path: .
        extra_requirements:
        - docs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;publish-the-package-on-pypi&#34;&gt;Publish the package on PyPI&lt;/h3&gt;
&lt;p&gt;This is as easy as &lt;code&gt;poetry publish&lt;/code&gt;! First, we&amp;rsquo;ll have to setup our PyPI whic we can do by &lt;a href=&#34;https://python-poetry.org/docs/repositories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;following the Poetry documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additional-tasks&#34;&gt;Additional tasks&lt;/h2&gt;
&lt;p&gt;Sphinx can automatically generate documentation for the modules, classes, and functions that have properly formatted docstrings. There are two main docstring styles: NumPy and Google. I use Google&amp;rsquo;s docstring format becaues it takes up less vertical space. The essential Sphinx extensions are autodoc (for automatically making docstrings into reStructuredText) and napoleon (for docstring formats). Both should be added to the Sphinx conf.py file as extensions,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;extensions = [&#39;sphinx.ext.autodoc&#39;,&#39;sphinx.ext.napoleon&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No installation by Poetry is necessary because both are part of the base installation of Sphinx.&lt;/p&gt;
&lt;p&gt;A common point of troubleshooting is that the readthedocs servers do not have your desired library installed. You will need to go to Advanced Settings on readthedocs and make sure you select to use both&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;Install Project
    Install your project inside a venv using setup.py install

Use system packages
    Give the venv access to the global site packages dir
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Technically, you may only need the second option to get e.g. numpy which readthedocs has installed on their servers for you. But if you want a more advanced option like sklearn that isn&amp;rsquo;t on the default servers, you&amp;rsquo;ll need to install the project. This means you need one more file at the top level of your project called .readthedocs.yaml which looks something like&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-TOML&#34;&gt;    version: 2

    build:
      image: latest
    
    python:
      version: 3.7
      install:
        - method: pip
          path: .
          extra_requirements:
            - docs
    
    sphinx:
      configuration: docs/conf.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This file makes sure that the setup.py command (the old package tool) interacts with the poetry configuration files correctly. Currently, there are some changes to python standards moving in poetry&amp;rsquo;s direction, but these are not implemented in readthedocs yet. Hence, this extra file.&lt;/p&gt;
&lt;h3 id=&#34;adding-jupyter-notebooks-to-the-docs&#34;&gt;Adding Jupyter notebooks to the docs&lt;/h3&gt;
&lt;p&gt;The key tool here is nbsphinx. This will need to be installed by poetry. Also, an ipython kernel and a jupyter reader will need to be installed for readthedocs to run the notebook (explicitely, &lt;code&gt;poetry add ipykernel&lt;/code&gt; and &lt;code&gt;poetry add jupyter_client&lt;/code&gt;. You can make these optional. You will also need the .readthedocs.yaml file so see the note at the end of the autodoc section.&lt;/p&gt;
&lt;h3 id=&#34;adding-a-license&#34;&gt;Adding a LICENSE&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Create a file in docs called license.rst and give the file a header like&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;License
=======

...
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Inside the index.rst look for&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;.. toctree::
    :maxdepth: 2
    :caption: Contents:
    license
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where we have added license to link the license file to the main documentation page (the name of the link will reflect the headings/subheadings in the file license.rst).&lt;/p&gt;
&lt;h2 id=&#34;cython-development&#34;&gt;Cython Development&lt;/h2&gt;
&lt;p&gt;I think the most effective cython tutorial is &lt;a href=&#34;https://cython.readthedocs.io/en/latest/src/userguide/wrapping_CPlusPlus.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this cython documentation example&lt;/a&gt;. You&amp;rsquo;ll eventually be introduced to the very basic example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setuptools import setup

from Cython.Build import cythonize

setup(ext_modules=cythonize(&amp;quot;rect.pyx&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example is good because it shows the essential features. However, if you have multiple c++ files that you want to compile together, you will need more. Eventually you&amp;rsquo;ll want to create more complicated objects to pass to cythonize/ext_modules. This will &lt;a href=&#34;https://docs.python.org/3/extending/building.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduce you to distutils&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main change to have cython code is to add a build.py file. This file uses the python library distutils to link all the c++ files and call cython. I have an &lt;a href=&#34;https://github.com/andgoldschmidt/pyprotoclust/blob/master/build.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example build.py on my Github&lt;/a&gt;. To get Poetry to use your build.py, you need to include &lt;code&gt;{build = &#39;build.py&#39;}&lt;/code&gt; under the &lt;code&gt;[tool.poetry]&lt;/code&gt; section in your pyproject.toml.&lt;/p&gt;
&lt;p&gt;For readthedocs, it seems that a wrapper around this build.py script is needed. A short setup.py script can be written to do this. You might want to look at additional tasks for some context on parts of this, but here&amp;rsquo;s an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Wrapper over build.py for readthedocs
from distutils.core import setup
from build import build

global setup_kwargs

setup_kwargs = {}

build(setup_kwargs)
setup(**setup_kwargs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another issue to address with readthedocs is making sure autodoc works for cython code. This fix is courtesy of &lt;a href=&#34;https://stackoverflow.com/questions/13238736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stackoverflow.com/questions/13238736&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;using-docker&#34;&gt;Using Docker&lt;/h2&gt;
&lt;p&gt;The goal of this section is to build and distribute a C++ shared package called example_package inside a Python wheel using poetry and auditwheel. For this, I followed: &lt;a href=&#34;https://github.com/riddell-stan/poetry-install-shared-lib-demo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/riddell-stan/poetry-install-shared-lib-demo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The wheel created using these instructions conforms to the manylinux2014 standard and should be usable on most Linux systems. This README also includes notes which may be of interest to developers seeking to understand how the &lt;code&gt;auditwheel repair&lt;/code&gt; command works. You&amp;rsquo;ll need to install &lt;a href=&#34;https://docs.docker.com/get-docker/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker&lt;/a&gt; (so we can use PyPA&amp;rsquo;s &lt;code&gt;manylinux2014&lt;/code&gt; build image).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Control basics with CartPole</title>
      <link>https://andgoldschmidt.github.io/posts/cartpole/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/cartpole/</guid>
      <description>&lt;p&gt;In this &lt;a href=&#34;https://github.com/andgoldschmidt/cartpole&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project on Github&lt;/a&gt; I coded up some tutorial concepts in control theory like observability, controllability, and the linear quadratic regulator using the example environment of a linear pendulum fixed to a cart. I also made fun Jupyter notebook movies to visualize the results.&lt;/p&gt;
&lt;p&gt;Also included under this project are the slides I did for a short class project covering reinforcement learning (RL) for CartPole from the OpenAI lab. It&amp;rsquo;s nice to contextualize model-free RL methods for control within a familiar environment where we have covered the control theory basics.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TODO: Turn this into a Google Colab&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://colab.research.google.com/&lt;/a&gt;
Embedding the notebook as a static webpage means there&amp;rsquo;s not a lot of fun to have with the movies so come back soon for a dynamic version of this post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://andgoldschmidt.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
