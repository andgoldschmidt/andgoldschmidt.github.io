<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Andy Goldschmidt</title>
    <link>https://andgoldschmidt.github.io/project/</link>
      <atom:link href="https://andgoldschmidt.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Andy Goldschmidt</copyright><lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://andgoldschmidt.github.io/media/icon_hufbcf978b363d0f47c9794b839e91d566_21519_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://andgoldschmidt.github.io/project/</link>
    </image>
    
    <item>
      <title>Spectral dynamic mode decomposition</title>
      <link>https://andgoldschmidt.github.io/project/spectral_dmd/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/project/spectral_dmd/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Spectral dynamic mode decomposition&lt;/strong&gt; is my term for Algorithm 1 from the paper &lt;em&gt;From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction&lt;/em&gt; by H. Lange, S.L. Brunton, J.N. Kutz (&lt;a href=&#34;https://www.youtube.com/watch?v=RBYFsFr4soo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;) (&lt;a href=&#34;https://arxiv.org/abs/2004.00574&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;). Necessary background is a familiarity with the dynamic mode decomposition (&lt;a href=&#34;https://www.youtube.com/watch?v=sQvrK8AGCAo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;) (&lt;a href=&#34;https://en.wikipedia.org/wiki/Dynamic_mode_decomposition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wikipedia entry&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This project walks through a toy example I coded up in Python. The example is similar to one from the paper which gets across the main points. I have packed away the main functions for the algorithm in the utility &lt;code&gt;spectral_help.py&lt;/code&gt; which has been linked in this project&amp;rsquo;s description.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from numpy.linalg import norm, solve

import matplotlib.pyplot as plt
cmap = plt.get_cmap(&#39;tab20&#39;)

from spectral_help import *
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;example-definition&#34;&gt;Example Definition&lt;/h1&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class toy_data:
    &amp;quot;&amp;quot;&amp;quot; 
    Generate toy oscillation data at specific frequencies.
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, tf, npts, noise):
        &amp;quot;&amp;quot;&amp;quot;
        Parameters:
            tf: final time
            npts: number of samples between 0 and tf
            noise: standard deviation of additive Gaussian noise
        &amp;quot;&amp;quot;&amp;quot;
        self.tf = tf
        self.npts = npts
        self.ts = np.linspace(0, self.tf, self.npts)
        self.dt = self.ts[1] - self.ts[0]
        self.noise = noise
        
        # Manual toy data (a stack of sines)
        self.freqs = [0.5, 2, 0.75, 3]
        self.X_fn = lambda ts: np.vstack([(np.sin([2 * np.pi * self.freqs[0] * ts]) + np.sin([2 * np.pi * self.freqs[1] * ts])),
                                          (np.sin([2 * np.pi * self.freqs[2] * ts]) + np.sin([2 * np.pi * self.freqs[3] * ts]))])
        
        # Normalize, add noise
        X = self.X_fn(self.ts)
        self.X_mean = np.mean(X, axis=1).reshape(-1,1)
        self.X_std = np.std(X, axis=1).reshape(-1,1)
        X = (X - self.X_mean)/self.X_std
        self.X_true = np.copy(X)
        self.X = X + np.random.randn(*X.shape)*self.noise
        
        # Construct test vars
        self.ts_test = None
        self.X_test = None
        
        
    def run_test(self, tf_predict, npts_predict):
        &amp;quot;&amp;quot;&amp;quot;
        Parameters:
            tf_predict: final time
            npts_predict: number of points between 0 and tf_predict
            
        Updates:
            self.ts_test: test time series
            self.X_test_true: normalized ground truth for the test
        &amp;quot;&amp;quot;&amp;quot;
        self.ts_test = np.linspace(0, tf_predict, npts_predict)
        X_test = self.X_fn(self.ts_test)
        
        # Use training std_dev and mean
        self.X_test_true = (X_test - self.X_mean)/self.X_std 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;run-the-experiment&#34;&gt;Run the Experiment&lt;/h1&gt;
&lt;p&gt;We run the spectral dynamic mode decomposition algorithm to learn the frequencies of an operator generating our toy time-series data. Because of the nature of the algorithm, we can do this with very noisy data (Here, we set the standard deviation at 0.5 for mean-zero variance-one toy training data).&lt;/p&gt;
&lt;p&gt;First, we configure the model. Then we set up the algorithm and run the optimization. The optimization is performed using the Accelerated Proximal Gradient Descent Method, or AGPD. The reason is because we are imposing sparsity using a $\ell_1$ norm&amp;ndash;that is, a LASSO-type optimization.&lt;/p&gt;
&lt;h2 id=&#34;1-configure-the-model&#34;&gt;1. Configure the model&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Config
# ======
np.random.seed(1)

# Data params
npts = 400             # number of time points
tf = 8                 # max time
std_noise = .5         # set the amount of noise on the mean-0, variance-1 data
predict_factor = 3     # prediction goes out to factor*T
exper = toy_data(tf, npts, std_noise)

# Algorithm params
freq_dim = 24          # freq. for algo to try
learning_rate = 1e-3   # LR = 1/beta from beta-smooth obj. bound (at least ideally--I&#39;m just choosing a number here)
reg_factor = 5         # regularization on sparsity

# SVD parameters 
threshold_type = &#39;percent&#39; # choose &#39;count&#39; for number or &#39;percent&#39; for s/max(s) &amp;gt; threshold
threshold = 1e-1

# Plot toggle
print_omega_updates = True
def print_update(omg, title):
    if print_omega_updates:
        print(&#39;{} $\omega$:\t&#39;.format(title), np.sort(np.round(omg[omg.astype(bool)], 3)))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;2-set-up-the-algorithm--3-optimize&#34;&gt;2. Set up the algorithm &amp;amp; 3. Optimize&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Algorithm
# =========
# 1. Initialize
omega = np.zeros(freq_dim)*2
print_update(omega, &#39;Initial&#39;)
A = np.random.rand(exper.X.shape[0], freq_dim*2)

obj_his = []
err_his = []

# 2. FFT to obtain the initial starting point for the optimization
for ifreq in range(len(omega)):
    # - Construct the residual via the current frequencies
    res = residual_j(ifreq, exper.X, A, omega, exper.ts)

    # - Select the maximum fft frequency as the initial value
    omega[ifreq] = max_fft_update(res, exper.dt)

    # - Update A
    A = update_A(exper.X, omega, exper.ts, threshold, threshold_type)

# 3. Perform proximal gradient descent from the initial point
# - Construct optimization functions
lam_cs = reg_factor*norm(A.T.dot(exper.X), np.inf)
def f(w):
    return loss(exper.X, A, w.flatten(), exper.ts)
def gradf(w):  
    return grad_loss(exper.X, A, w.flatten(), exper.ts)
def func_g(w):
    return lam_cs*np.linalg.norm(w, ord=1)
def prox_g(w, t):
    res = []
    r = t*lam_cs
    for wi in w.flatten():
        if wi &amp;gt; r:
            res.append(wi-r)
        elif wi &amp;lt; -r:
            res.append(wi+r)
        else:
            res.append(0)
    return np.array(res)

# - Optimization algorithm
w, iobj_his, ierr_his, cond = optimizeWithAPGD(omega.reshape(1,-1), f, func_g, gradf, prox_g, (1/learning_rate)*npts, max_iter=5000, verbose=True)
obj_his.append(iobj_his)
err_his.append(ierr_his)
omega = w
print_update(omega, &#39;Final  &#39;)

# 4. Final operator update
A = update_A(exper.X, omega, exper.ts, threshold, threshold_type)
print_update(np.array(exper.freqs), &#39;Expected&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Initial $\omega$:	 []
Final   $\omega$:	 [0.494 0.743 1.991 2.99 ]
Expected $\omega$:	 [0.5  0.75 2.   3.  ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We printed out the frequencies (previous cell). In the next cell, we show the objective value and the gradient of the objective for the iterations of the optimization. The characteristic oscillations of an accelerated gradient descent are observed.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot 
# ====
# Inspect convergence results
fig,axes = plt.subplots(2,1,figsize=[10,10])
ax = axes[0]
ax.plot(iobj_his)
ax.set_ylabel(&#39;Obj. value&#39;)
ax.set_xlabel(&#39;Iterations&#39;)
ax.set_yscale(&#39;log&#39;)
ax = axes[1]
ax.plot(ierr_his)
ax.set_ylabel(&#39;Gradient magn.&#39;)
ax.set_xlabel(&#39;Iterations&#39;)
ax.set_yscale(&#39;log&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_da992a4cde976d6430ac9d6838d40a67.png 400w,
               /project/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_fa9ef638480d69f3b09d71ebdc3dc0df.png 760w,
               /project/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/project/spectral_dmd/output_8_0_hu30a7000c7500c37f930b556e7840238b_35632_da992a4cde976d6430ac9d6838d40a67.png&#34;
               width=&#34;635&#34;
               height=&#34;601&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;result&#34;&gt;Result&lt;/h1&gt;
&lt;p&gt;We see the training (top) and test (bottom) results for our two-dimensional multi-frequency dynamics.&lt;/p&gt;
&lt;p&gt;Notice the order of magnitude increase in the horizontal time axis on the bottom plot. On this plot, we observe that the solutions match the test simulations and are stable because of the model assumptions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Make prediction
exper.run_test(tf*predict_factor, 10*npts) # keep sample freq the same

bigOmg_test = BigOmg(omega, exper.ts_test)
X_pred = A@(bigOmg_test)


# Plot data
fig,axes = plt.subplots(1,2,figsize=[20,3])
fig.subplots_adjust(hspace=0.3, wspace=0.2)
leg_params = {&#39;loc&#39;: &#39;upper right&#39;, &#39;shadow&#39;: True, &#39;fancybox&#39;: True}

ax = axes[0]
ax.plot(exper.ts, exper.X_true[0], color=cmap(1), label=&#39;Simulation&#39;)
ax.plot(exper.ts, exper.X[0], ls=&#39;&#39;, marker=&#39;+&#39;, color=cmap(0), label=&#39;Training Data&#39;)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;x&#39;)
ax.legend(**leg_params)
ax = axes[1]
ax.plot(exper.ts, exper.X_true[1], color=cmap(3), label=&#39;Simulation&#39;)
ax.plot(exper.ts, exper.X[1], ls=&#39;&#39;, marker=&#39;+&#39;, color=cmap(2), label=&#39;Training Data&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;y&#39;)
ax.set_ylim([-3.5,3.5])

# Plot model
fig,axes = plt.subplots(2,1,figsize=[20,5])
fig.subplots_adjust(hspace=0.3, wspace=0.2)
leg_params = {&#39;loc&#39;: &#39;upper right&#39;, &#39;shadow&#39;: True, &#39;fancybox&#39;: True}

ax = axes[0]
ax.plot(exper.ts_test, exper.X_test_true[0], color=cmap(1), label=&#39;Test Simulation&#39;)
ax.plot(exper.ts_test, X_pred[0], ls=&#39;-&#39;, color=cmap(0), label=&#39;Model Prediction&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;x&#39;)
ax = axes[1]
ax.plot(exper.ts_test, exper.X_test_true[1], color=cmap(3), label=&#39;Test Simulation&#39;)
ax.plot(exper.ts_test, X_pred[1], ls=&#39;-&#39;, color=cmap(2), label=&#39;Model Prediction&#39;)
ax.legend(**leg_params)
ax.set_xlabel(&#39;t&#39;)
ax.set_ylabel(&#39;y&#39;)
ax.set_ylim([-3.5,3.5])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Click on the figures to zoom.)&lt;/p&gt;
&lt;p&gt;Training data:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_0aaee56126f71e3c3b2081ee028f035b.png 400w,
               /project/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_40f177da89af1e202d5e880b3bcb392e.png 760w,
               /project/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/project/spectral_dmd/output_10_1_hud71e81e40d92cfc4dd622625fec7d59d_50350_0aaee56126f71e3c3b2081ee028f035b.png&#34;
               width=&#34;760&#34;
               height=&#34;142&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Test data:
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_e7a65ff9c7a4bf3ef0623c0f8f182904.png 400w,
               /project/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_bec5f35f2817b20d7d9e4c24bafd0ffa.png 760w,
               /project/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/project/spectral_dmd/output_10_2_hu06f9e836a8ac563a2819214fbf73255e_110427_e7a65ff9c7a4bf3ef0623c0f8f182904.png&#34;
               width=&#34;760&#34;
               height=&#34;212&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Derivative</title>
      <link>https://andgoldschmidt.github.io/project/derivative/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/project/derivative/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Derivative&lt;/strong&gt; is an open-source project I started in 2019-2020 that turned into a collaboration with Markus Quade (Github, &lt;a href=&#34;https://github.com/Ohjeah&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Ohjeah&lt;/a&gt;) and Brian de Silva (Github, &lt;a href=&#34;https://github.com/briandesilva&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@briandesilva&lt;/a&gt;). It is a standalone suite of numerical differentiation methods for noisy time series data written in Python.&lt;/p&gt;
&lt;p&gt;The goal is to provide some common numerical differentiation techniques that showcase improvements that can be made on finite differences when data is noisy. The package binds these common differentiation methods to a single easily implemented differentiation interface to encourage user adaptation.&lt;/p&gt;
&lt;p&gt;Derivative is a contribution to &lt;a href=&#34;https://github.com/dynamicslab/pysindy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PySINDy&lt;/a&gt;&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;
&lt;td&gt; &lt;a href=&#34;https://zenodo.org/badge/latestdoi/186055899&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/186055899.svg&#34; style=&#34;width: 200px;&#34; alt=&#34;DOI&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;td&gt; &lt;a href=&#34;https://pysindy.readthedocs.io/en/latest/?badge=latest&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/pysindy/badge/?version=latest&#34; style=&#34;width: 100px;&#34; alt=&#34;Documentation Status&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;td&gt; &lt;a href=&#34;https://github.com/dynamicslab/pysindy/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dynamicslab/pysindy.svg?style=social&amp;label=Star&amp;maxAge=2592000&#34; style=&#34;width: 100px;&#34; alt=&#34;GitHub stars&#34;&gt; &lt;/a&gt; &lt;/td&gt; 
&lt;/tr&gt;&lt;/table&gt;
&lt;p&gt;PySINDy is an open source Python package for the Sparse Identification of Nonlinear Dynamical systems (SINDy).&lt;/p&gt;
&lt;p&gt;At some point, I&amp;rsquo;ll write a post about my version of total variational regularization (see the figure above). I adapted a technique from &lt;em&gt;The solution path of the generalized lasso&lt;/em&gt; (&lt;a href=&#34;https://projecteuclid.org/journals/annals-of-statistics/volume-39/issue-3/The-solution-path-of-the-generalized-lasso/10.1214/11-AOS878.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.1214/11-AOS878&lt;/a&gt;) by R.J. Tibshirani &amp;amp; J. Taylor to write a nice variation of the classic algorithm in &lt;em&gt;Numerical Differentiation of Noisy, Nonsmooth Data&lt;/em&gt; (&lt;a href=&#34;https://doi.org/10.5402/2011/164564&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.5402/2011/164564&lt;/a&gt;) by Rick Chartrand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical Clustering with Prototypes</title>
      <link>https://andgoldschmidt.github.io/project/pyprotoclust/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/project/pyprotoclust/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Pyprotoclust&lt;/strong&gt; is an implementatin of representative hierarchical clustering using minimax linkage. The original algorithm is from &lt;em&gt;Hierarchical Clustering With Prototypes via Minimax Linkage&lt;/em&gt; (&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4527350/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DOI: 10.1198/jasa.2011.tm10183&lt;/a&gt;) by J. Bien and R. Tibshirani; Pyprotoclust takes a distance matrix as input. It returns a linkage matrix encoding the hierachical clustering as well as an additional list labelling the prototypes associated with each clustering.&lt;/p&gt;
&lt;p&gt;I coded up a fun example inspired by the original paper where I apply the algorithm to determine representative pictures for the Olivetti Faces dataset. It can be found &lt;a href=&#34;https://pyprotoclust.readthedocs.io/en/latest/notebooks/Example.html#Olivetti-Faces&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;in the Pyprotoclust documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure&lt;/strong&gt;: (Left) A dendrogram of the hierarchical clustering example with a dashed line at the example cut height. (Right) A scatter plot of the example with circles centered at prototypes drawn with radii equal to the top-level linkage heights of each cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python package development</title>
      <link>https://andgoldschmidt.github.io/project/poetry/</link>
      <pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/project/poetry/</guid>
      <description>&lt;p&gt;There are lots of features that go into packaging up Python code for other users. How can we make a project that ships easily to users and takes advantage of our normal development tools? We&amp;rsquo;ll discuss:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Poetry: for easily making and publishing a package&lt;/li&gt;
&lt;li&gt;Sphinx: for making documentation&lt;/li&gt;
&lt;li&gt;Readthedocs: free professional-looking documenation hosting and formatting&lt;/li&gt;
&lt;li&gt;PyCharm: the default Python IDE (you can get the professional version as a student)&lt;/li&gt;
&lt;li&gt;PyPI: Python pacakge index, where you store stuff that people can pip install.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;basics&#34;&gt;Basics&lt;/h2&gt;
&lt;p&gt;Read the &lt;a href=&#34;https://python-poetry.org/docs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Poetry docs&lt;/a&gt; to install. It&amp;rsquo;s good documentation; you should skim the Installation and Basic Usage first. For zsh users, make sure Poetry got added to your Path inside zshrc. For bash people, it&amp;rsquo;s automatic.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make a new project with Poetry.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a python project. (See &lt;em&gt;Choosing a project name&lt;/em&gt; below this list.)
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry new myproject
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Change to this directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Start the poetry virtual environment.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry shell
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will make a virtual environment that is like a fresh Python installation for us to be explicit about our package&amp;rsquo;s dependencies.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Install new pacakges as needed.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;poetry add python_package
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The name python_pacakge would be something like numpy. Poetry will install the package to the virtual environment and add the package to the pyproject.toml file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Take a look at the pyproject.toml file. All your package settings are here. Any added pacakges appear automatically. We also have a file not for human consumption called poetry.lock which does all the work of building the exact environement we are using. It can be good to commit this poetry.lock to version control so someone interacting with our package knows the exact packages we were using.&lt;/li&gt;
&lt;li&gt;To update all pacakges added with &lt;em&gt;poetry add&lt;/em&gt;, run &lt;em&gt;poetry update&lt;/em&gt;. To install the current system and update the poetry.lock file, run &lt;em&gt;poetry install&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;choosing-a-project-name&#34;&gt;Choosing a project name&lt;/h3&gt;
&lt;p&gt;A new poetry project ``myproject&#39;&#39; has a specific directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    myproject
    |-- pyproject.toml
    |-- README.rst
    |-- myproject
    |   |-- __init__.py
    |-- tests
        |-- __init__.py
        |-- test_myproject.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The project name is for the top level directory of the project. This is the name of the github repository and the PyPI project so it is invoked with &lt;code&gt;pip install myprojectname&lt;/code&gt;. It should be unique. Project names on PyPI should NOT use dashes (&lt;a href=&#34;https://stackoverflow.com/questions/8350853/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stackoverflow.com/questions/8350853/)&lt;/a&gt;. Underscores are allowed but discouraged. The package or module name is the inner directory containing __init__.py. This is the code that will be invoked by the user as &lt;em&gt;import my_project_name&lt;/em&gt; in code. It does not have to be unique. It can use underscores.Note that Poetry defaults to matching project and package names. This is also the Python style guideline (PEP 423).&lt;/p&gt;
&lt;h3 id=&#34;version-control&#34;&gt;Version control&lt;/h3&gt;
&lt;p&gt;Now we start tracking our new package on version control. Init a git repo in the project directory. Do this in your usual way. (e.g. hosting on github). The splash page for your package will be README.rst! Make it pretty.&lt;/p&gt;
&lt;h3 id=&#34;pycharm&#34;&gt;PyCharm&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s use a modern IDE. Open up the project in PyCharm.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Get the location of the interpreter for this virtual environment for PyCharm. Run this command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;poetry run which python
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make the virtual environment default for PyCharm.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Settings $\rightarrow$ Project $\rightarrow$ Python Interpreter&lt;/li&gt;
&lt;li&gt;Click the gear, and select add.&lt;/li&gt;
&lt;li&gt;Choose the option \textit{existing environment} and add the path to the poetry virtual environment. Apply changes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now PyCharm will complain when you try to use python code you haven&amp;rsquo;t added. PyCharm will also give you actions to import missing libraries. However, be sure to add the python libraries with poetry, not PyCharm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;add-documentation-with-sphinx&#34;&gt;Add documentation with Sphinx&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s set up Sphinx.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Start&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir docs
poetry add sphinx
cd docs
sphinx-quickstart
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The command line will prompt you with a few questions. Use the default settings, but enter any project-specific information as needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All Sphinx settings are in conf.py. The first setting to edit is the path. Uncomment the lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import os
import sys
sys.path.insert(0, os.path.abspath(&#39;.&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and change the &amp;ldquo;.&amp;rdquo; to &amp;ldquo;..&amp;rdquo; to reflect the docs folder.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make sure that Sphinx knows that the main file is index.rst by adding the lines&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Assign the master document
master_doc = &#39;index&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to conf.py.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Test to see that your docs compile. Run the command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;make html
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;inside the docs folder then open up index.html in your web browser.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add docs to readthedocs.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href=&#34;https://readthedocs.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;readthedocs&lt;/a&gt;, login, then find and click import.&lt;/li&gt;
&lt;li&gt;Paste the link to the github repo and create.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Readthedocs will find the conf.py file and build the documentation.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Check that github will let readthedocs know when the documentation is updated. Go to the project repo settings and confirm that the Webhooks tab includes readthedocs.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Note:
Oddly, the default Poetry config section tool.poetry.dependencies that allows users of your package to avoid installing development tools like Sphinx is not supported by readthedocs (the listed packages won&amp;rsquo;t be used). There is an alternative, e.g.&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sphinx = {version=&amp;quot;^3.0.2&amp;quot;, optional = true}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;To add packages to Poetry as optional you can call poetry add sphinx &amp;ndash;optional to autofill this format.
In your .readthedocs.yaml file, you can make sure these packages are installed by adding the extra_name parameter to extra_requirements, e.g.:&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python:
    version: 3.7
    install:
    - method: pip
        path: .
        extra_requirements:
        - docs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;publish-the-package-on-pypi&#34;&gt;Publish the package on PyPI&lt;/h3&gt;
&lt;p&gt;This is as easy as &lt;code&gt;poetry publish&lt;/code&gt;! First, we&amp;rsquo;ll have to setup our PyPI whic we can do by &lt;a href=&#34;https://python-poetry.org/docs/repositories/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;following the Poetry documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additional-tasks&#34;&gt;Additional tasks&lt;/h2&gt;
&lt;p&gt;Sphinx can automatically generate documentation for the modules, classes, and functions that have properly formatted docstrings. There are two main docstring styles: NumPy and Google. I use Google&amp;rsquo;s docstring format becaues it takes up less vertical space. The essential Sphinx extensions are autodoc (for automatically making docstrings into reStructuredText) and napoleon (for docstring formats). Both should be added to the Sphinx conf.py file as extensions,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;extensions = [&#39;sphinx.ext.autodoc&#39;,&#39;sphinx.ext.napoleon&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No installation by Poetry is necessary because both are part of the base installation of Sphinx.&lt;/p&gt;
&lt;p&gt;A common point of troubleshooting is that the readthedocs servers do not have your desired library installed. You will need to go to Advanced Settings on readthedocs and make sure you select to use both&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;Install Project
    Install your project inside a venv using setup.py install

Use system packages
    Give the venv access to the global site packages dir
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Technically, you may only need the second option to get e.g. numpy which readthedocs has installed on their servers for you. But if you want a more advanced option like sklearn that isn&amp;rsquo;t on the default servers, you&amp;rsquo;ll need to install the project. This means you need one more file at the top level of your project called .readthedocs.yaml which looks something like&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-TOML&#34;&gt;    version: 2

    build:
      image: latest
    
    python:
      version: 3.7
      install:
        - method: pip
          path: .
          extra_requirements:
            - docs
    
    sphinx:
      configuration: docs/conf.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This file makes sure that the setup.py command (the old package tool) interacts with the poetry configuration files correctly. Currently, there are some changes to python standards moving in poetry&amp;rsquo;s direction, but these are not implemented in readthedocs yet. Hence, this extra file.&lt;/p&gt;
&lt;h3 id=&#34;adding-jupyter-notebooks-to-the-docs&#34;&gt;Adding Jupyter notebooks to the docs&lt;/h3&gt;
&lt;p&gt;The key tool here is nbsphinx. This will need to be installed by poetry. Also, an ipython kernel and a jupyter reader will need to be installed for readthedocs to run the notebook (explicitely, &lt;code&gt;poetry add ipykernel&lt;/code&gt; and &lt;code&gt;poetry add jupyter_client&lt;/code&gt;. You can make these optional. You will also need the .readthedocs.yaml file so see the note at the end of the autodoc section.&lt;/p&gt;
&lt;h3 id=&#34;adding-a-license&#34;&gt;Adding a LICENSE&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Create a file in docs called license.rst and give the file a header like&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;License
=======

...
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Inside the index.rst look for&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-Markdown&#34;&gt;.. toctree::
    :maxdepth: 2
    :caption: Contents:
    license
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where we have added license to link the license file to the main documentation page (the name of the link will reflect the headings/subheadings in the file license.rst).&lt;/p&gt;
&lt;h2 id=&#34;cython-development&#34;&gt;Cython Development&lt;/h2&gt;
&lt;p&gt;I think the most effective cython tutorial is &lt;a href=&#34;https://cython.readthedocs.io/en/latest/src/userguide/wrapping_CPlusPlus.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this cython documentation example&lt;/a&gt;. You&amp;rsquo;ll eventually be introduced to the very basic example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from setuptools import setup

from Cython.Build import cythonize

setup(ext_modules=cythonize(&amp;quot;rect.pyx&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example is good because it shows the essential features. However, if you have multiple c++ files that you want to compile together, you will need more. Eventually you&amp;rsquo;ll want to create more complicated objects to pass to cythonize/ext_modules. This will &lt;a href=&#34;https://docs.python.org/3/extending/building.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introduce you to distutils&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main change to have cython code is to add a build.py file. This file uses the python library distutils to link all the c++ files and call cython. I have an &lt;a href=&#34;https://github.com/andgoldschmidt/pyprotoclust/blob/master/build.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;example build.py on my Github&lt;/a&gt;. To get Poetry to use your build.py, you need to include &lt;code&gt;{build = &#39;build.py&#39;}&lt;/code&gt; under the &lt;code&gt;[tool.poetry]&lt;/code&gt; section in your pyproject.toml.&lt;/p&gt;
&lt;p&gt;For readthedocs, it seems that a wrapper around this build.py script is needed. A short setup.py script can be written to do this. You might want to look at additional tasks for some context on parts of this, but here&amp;rsquo;s an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Wrapper over build.py for readthedocs
from distutils.core import setup
from build import build

global setup_kwargs

setup_kwargs = {}

build(setup_kwargs)
setup(**setup_kwargs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another issue to address with readthedocs is making sure autodoc works for cython code. This fix is courtesy of &lt;a href=&#34;https://stackoverflow.com/questions/13238736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stackoverflow.com/questions/13238736&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;using-docker&#34;&gt;Using Docker&lt;/h2&gt;
&lt;p&gt;The goal of this section is to build and distribute a C++ shared package called example_package inside a Python wheel using poetry and auditwheel. For this, I followed: &lt;a href=&#34;https://github.com/riddell-stan/poetry-install-shared-lib-demo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/riddell-stan/poetry-install-shared-lib-demo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The wheel created using these instructions conforms to the manylinux2014 standard and should be usable on most Linux systems. This README also includes notes which may be of interest to developers seeking to understand how the &lt;code&gt;auditwheel repair&lt;/code&gt; command works. You&amp;rsquo;ll need to install &lt;a href=&#34;https://docs.docker.com/get-docker/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;docker&lt;/a&gt; (so we can use PyPA&amp;rsquo;s &lt;code&gt;manylinux2014&lt;/code&gt; build image).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CartPole</title>
      <link>https://andgoldschmidt.github.io/project/cartpole/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/project/cartpole/</guid>
      <description>&lt;p&gt;In this &lt;a href=&#34;https://github.com/andgoldschmidt/cartpole&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project on Github&lt;/a&gt; I coded up some tutorial concepts in control theory like observability, controllability, and the linear quadratic regulator using the example environment of a linear pendulum fixed to a cart. I also made fun Jupyter notebook movies to visualize the results.&lt;/p&gt;
&lt;p&gt;Also included under this project are the slides I did for a short class project covering reinforcement learning (RL) for CartPole from the OpenAI lab. It&amp;rsquo;s nice to contextualize model-free RL methods for control within a familiar environment where we have covered the control theory basics.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TODO: Turn this into a Google Colab&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://colab.research.google.com/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
