<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Control | Andy Goldschmidt</title>
    <link>https://andgoldschmidt.github.io/tag/control/</link>
      <atom:link href="https://andgoldschmidt.github.io/tag/control/index.xml" rel="self" type="application/rss+xml" />
    <description>Control</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Andy Goldschmidt</copyright><lastBuildDate>Wed, 09 Mar 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://andgoldschmidt.github.io/media/icon_hufbcf978b363d0f47c9794b839e91d566_21519_512x512_fill_lanczos_center_3.png</url>
      <title>Control</title>
      <link>https://andgoldschmidt.github.io/tag/control/</link>
    </image>
    
    <item>
      <title>Automatically differentiating numerical integrators</title>
      <link>https://andgoldschmidt.github.io/posts/jax_mpc/</link>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/jax_mpc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;JAX&lt;/strong&gt; is a research project from Google. For our purpose, JAX is a way to do automatic differentiation: &lt;em&gt;With its updated version of Autograd, JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives.&lt;/em&gt; Numerical integration schemes are just native python and NumPy functions so JAX can automatically differentiate them. An example of when we might want to automatically differentiating numerical integrators is the control of nonlinear dynamical systems.&lt;/p&gt;
&lt;p&gt;In particular, the purpose of this post is to learn how to do three things.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use JAX.&lt;/li&gt;
&lt;li&gt;Implement a few numerical integration schemes.&lt;/li&gt;
&lt;li&gt;Use JAX to linearize a numerical integration scheme.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax.numpy as jnp
from jax import grad, jit, vmap, jacfwd, jacrev

import numpy as np
from tqdm.notebook import tqdm, trange
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
from matplotlib.cm import ScalarMappable
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;derivatives-with-jax&#34;&gt;Derivatives with JAX&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;JAX can compute derivatives through algorithms. A nice introduction is: &lt;a href=&#34;https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is a function, the rectified cube:
\begin{equation}
f(x) = |x|^3.
\end{equation}&lt;/p&gt;
&lt;p&gt;We can define $f(x)$ in a bit of a silly way by using an if statement.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def rectified_cube(x):
    r = 1
    if x &amp;lt; 0.:
        for i in range(3):
            r *= x
            r = -r
    else:
        for i in range(3):
            r *= x
    return r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;JAX can differentiate this $f(x)$ no problem.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gradient_function = grad(rectified_cube)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig, ax = plt.subplots(1)
xs = np.linspace(-1, 1)
fx = []
d_fx = []
for x in xs:
    fx.append(rectified_cube(x))
    d_fx.append(gradient_function(x))
ax.plot(xs, fx, xs, d_fx, lw=2)
ax.legend([&#39;$|x|^3$&#39;, &#39;$\\frac{d}{dx} |x|^3$&#39;], fontsize=14, ncol=2)
ax.set_xlabel(&#39;x&#39;, fontsize=14);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_15ae8a3e28aa18ad12e25427c66ae0ea.png 400w,
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_73dd0e5af9ed8e0bac93fc01c34c1d85.png 760w,
               /posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_8_0_hu8143b5dc0a8d9ee4a48399ab4dbba28b_12050_15ae8a3e28aa18ad12e25427c66ae0ea.png&#34;
               width=&#34;370&#34;
               height=&#34;266&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;numerical-integration--autodiff&#34;&gt;Numerical integration + Autodiff&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;Here is a continuous model:
\begin{equation}
\dot{x}(t) = f(x(t), u(t)).
\end{equation}&lt;/p&gt;
&lt;p&gt;In many situations in computing (like model predictive control), the continuous dynamics must be converted to a discrete model
\begin{equation}
x_{k + 1} = F(x_k, u_k).
\end{equation}&lt;/p&gt;
&lt;p&gt;The reason for the conversion is that controls are computed as a zero-order-hold over discrete time intervals. The full discrete list of control amplitudes can be optimized. In the continuous limit, the discrete list becomes a function. Working with functions is much harder and doesn&amp;rsquo;t allow for a scheme like MPC which depends on &lt;em&gt;taking a step&lt;/em&gt;. Plus, you can often use a simple basis to approximate function dynamics within the discrete time step (how?).&lt;/p&gt;
&lt;p&gt;The RHS term $F(x_k, u_k)$ is a numerical integration. There are many ways to do this. $F$ is frequently nonlinear. Unfortunately, we really only know how to do MPC for systems with linear discrete dynamics where
\begin{equation}
F(x_k, u_k)=A x_k + B u_k.
\end{equation}
In order to do more interesting systems, we rely on locally linear approximations of $F$ in algorithms. This means the model is local about some guess trajectory, i.e. you compute matrices like
\begin{equation}
A \equiv \nabla_x F(x, u)|_{x_g}
\end{equation}
and multiply them with $x - x_g$ (it&amp;rsquo;s just the 1st order Fourier expansion). In MPC, the choice for $x_g$ (read: x-guess) is often a recently valid solution that&amp;rsquo;s been shifted to the left to accommodate the next prediction horizon. In any case, this means we want derivatives of $F$. This is where JAX comes in: &lt;strong&gt;If we know the continuous model $f$ and the algorithm $A$ we used to compute the numerical integration (i.e. $F = A \circ f$), we can find these linear approximations with automatic differentiaton.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Why might this be better? It&amp;rsquo;s hard (or at a minumum, annoying) to compute an analytic linearization of some numerical integrators $F$ even for simple nonlinear dynamics.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Exercise: Are there cases where it might not be reasonable to explicitely compute derivatives?&lt;/em&gt;&lt;/p&gt;
&lt;!--- Answer: Derivatives are annying even for a simple example. For instance, try a bilinear system with Euler integration. It&#39;s still doable. Meanwhile, it is probably unreasonable to compute explicit derivatives for a neural network description of the dynamics. ---&gt;
&lt;h2 id=&#34;van-der-pol-experiment&#34;&gt;Van der Pol experiment&lt;/h2&gt;
&lt;p&gt;In this section, we&amp;rsquo;ll review some numerical integrators as preparation for thinking about how we might locally linearize them with JAX.&lt;/p&gt;
&lt;p&gt;The toy system we will use is a driven Van der Pol oscillator,
\begin{equation}
\begin{aligned}
&amp;amp;\dot{x}_1 = x_2, \&lt;br&gt;
&amp;amp;\dot{x}_2 = -x_1 + \mu (1 - x_1^2) x_2 + u
\end{aligned}
\end{equation}&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vdp(t, x, u):
        mu = 2
        x1, x2 = x
        return jnp.array([
            x2,
            -x1 + mu * (1 - x1 ** 2) * x2 + u
        ])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;euler-integration&#34;&gt;Euler integration&lt;/h2&gt;
&lt;p&gt;The simplest choice for numerical integration is Euler integration, which combines the definition of the derivative
\begin{equation}
\dot{x}(t) = \lim_{\Delta t \rightarrow 0} \frac{\Delta x}{\Delta t}
\end{equation}
with the dynamics $\dot{x}(t) = f(t, x(t), u(t))$ such that
\begin{equation}
\frac{x_{k+1} - x_k}{\Delta t} \approx f(k, x_k, u_k)
\end{equation}
so
\begin{equation}
x_{k+1} \approx x_k + \Delta t f(k, x_k, u_k) \equiv F(x_k, u_k)
\end{equation}&lt;/p&gt;
&lt;p&gt;Set $z_k = [x_k, u_k]$ for simplicity.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def euler(z, dt=1):
    return z[:2] + dt * vdp(_, z[:2], z[2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Driving is external; set a policy.
def u_fn(t):
    return jnp.zeros_like(t)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One fun thing to do is break the numerical integration by taking steps that are too big. Do that by dividing the interval [0,15] into 100 steps.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate the oscillator.
ts = jnp.linspace(0, 15, 100)
dt = ts[1] - ts[0]

t0 = ts[0]
x0 = jnp.array([[1], [-2]])

xs = [None] * (len(ts) + 1)
xs[0] = x0
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[i], u_fn(t)])
    xs[i + 1] = euler(z, dt)
xs = jnp.hstack(xs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1, x2 = xs
fig, ax = plt.subplots(1, figsize=[8,8])
ax.plot(x1, x2, lw=5)
ax.set_aspect(&#39;equal&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_e07c234137738ed42c9e41d2fa6dc3e6.png 400w,
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_ec645fb030a4e79f024a3168eaaf15ac.png 760w,
               /posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_22_0_hue69bbd6c2d49dea55bf72d162e1c0cb2_19379_e07c234137738ed42c9e41d2fa6dc3e6.png&#34;
               width=&#34;275&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We can compute the Jacobian of $F(x_k, u_k)$ using JAX. That is, we want to compute the derivative to arrive at a matrix, $\nabla_z F(z)$. There are a few ways to do this with JAX.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Differentiate (retain ability to use other args)
jac_euler = jacfwd(euler)

# Is prefixing values faster? Not by much.
jac_euler_2 = jacfwd(lambda z: euler(z, dt))

# Is backward faster? No way! Recall: Why not?
jac_euler_3 = jacrev(euler)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jac_xs = [None] * len(ts)
det_jac_xs = [None] * len(ts)
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[:, i][:, None], u_fn(t)])
    jac_xs[i] = jac_euler(z.squeeze(), dt)
    det_jac_xs[i] = np.linalg.det(jac_xs[i][:, :2])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$\nabla_z F(z)$ is a matrix. Our goal in a first order linearization is to compute something like $\Delta F = \nabla_z F(z) \cdot \Delta z$. Here, let&amp;rsquo;s do that for the $x$ values.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the differential of F
dx = xs[:, 1:] - xs[:, :-1]
df = np.vstack([jac_xs[i][:,:2] @ dx[:, i] for i in range(len(ts))]).T
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the derivatives are only as good as our numerical integration (which we already decided to break). Keep in mind that this is the behavior you would get even if you did analytic derivatives with respect to the Euler integration scheme.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tangent vectors colored by the determinant of the Jacobian
fig, ax = plt.subplots(1, figsize=[8, 8])
cmap = plt.get_cmap(&#39;viridis&#39;)
norm = plt.Normalize(min(det_jac_xs), max(det_jac_xs))
for i in range(len(ts)):
    ax.arrow(x1[i], x2[i], df[0][i], df[1][i], width=.05,
             color=cmap(norm(det_jac_xs[i])))
ax.set_aspect(&#39;equal&#39;)
fig.colorbar(ScalarMappable(norm=norm, cmap=cmap))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_fb708b83c3764acf83b599426db472f7.png 400w,
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_56bd4bcfecec0840b15fe735c0e66bde.png 760w,
               /posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_29_1_hub5c88f3ef96019b9183e27e037120f77_32601_fb708b83c3764acf83b599426db472f7.png&#34;
               width=&#34;359&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;If you want to peak at the matrix assoicated to $\nabla_z F(z)$, then you need to act on basis vectors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basis = [None] * (xs.shape[0] + 1)
for i in range(len(basis)):
    basis[i] = np.zeros([xs.shape[0] + 1, 1])
    basis[i][i] = 1
    
z0 = jnp.vstack([xs[:, 0][:, None], u_fn(t)])
np.hstack([jac_euler(z0.squeeze(), dt) @ b for b in basis])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[1.        , 0.15151516, 0.        ],
       [1.0606061 , 1.        , 0.15151516]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;runge-kutta-methods&#34;&gt;Runge-Kutta methods&lt;/h2&gt;
&lt;p&gt;One nice thing with JAX is that if we pick more interesting numerical integration schemes, we don&amp;rsquo;t have to worry about computing the analytic derivative of the more complicated function $F(x_k, u_k)$. Let&amp;rsquo;s see this in action.&lt;/p&gt;
&lt;p&gt;Generalize the Euler method so that
\begin{equation}
x_{k+1} \approx x_k + \Delta t \phi(k, x_k, u_k) \equiv F(x_k, u_k)
\end{equation}
with $\phi$ some new function defined to reduce errors by leveraging intermediate evaluations of $f$.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m not going to derive it, but one way to improve things is the classic 4th order Runge-Kutta:&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{k+1} \approx x_k + \frac{\Delta t}{6} (f_1 + 2 f_2 + 2 f_3 + f_4)
\end{equation}
where&lt;/p&gt;
&lt;p&gt;$f_1 = f(t_k, x_k, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_2 = f(t_k + \Delta t / 2, x_k + f_1 \Delta t / 2, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_3 = f(t_k + \Delta t / 2, x_k + f_2 \Delta t / 2, u_k)$,&lt;/p&gt;
&lt;p&gt;$f_4 = f(t_k + \Delta t, x_k +  f_3 \Delta t, u_k)$.&lt;/p&gt;
&lt;p&gt;Why do we modify the $x$ arguments in the function, but not the $u$ arguments? First off, we assume $f$ is a known function, but we don&amp;rsquo;t know anything about the control policy. Our control assumption is actually zero-order-hold so we are assuming $u_k$ won&amp;rsquo;t vary within our step. This is pretty important for the validity of the scheme. Luckily we&amp;rsquo;re in charge of the control so we can set this.&lt;/p&gt;
&lt;p&gt;For completeness, let&amp;rsquo;s just say we don&amp;rsquo;t make the assumption of zero order hold. Then the Runge-Kutta approximation is not going to satisfy the promised accuracy, and we don&amp;rsquo;t have any way to improve the integration by cancelling Fourier terms like we did with $x$ (this is because $u$ is not constrained by a function). One final way to think about this: if we did know the policy, then $f(t, x, u) \mapsto f(t, x, \pi(t, x)) \equiv f^\pi(t, x)$ and we can have no gradients with respect to $u$ because it is fixed by the policy.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Again, we don&#39;t have explicit time dependence. 
# Exercise: How would things change?
def rk4(z, dt=1):
    f1 = vdp(_, z[:2], z[2])
    f2 = vdp(_, z[:2] + f1 * dt / 2, z[2])
    f3 = vdp(_, z[:2] + f2 * dt / 2, z[2])
    f4 = vdp(_, z[:2] + f3 * dt, z[2])
    return z[:2] + (f1 + 2 * f2 + 2 * f3 + f4) * dt / 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pick the same time scheme as before, but now we&amp;rsquo;re going to get a much better numerical integration with our improved method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Simulate the oscillator.
xs = [None] * (len(ts) + 1)
xs[0] = x0
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[i], u_fn(t)])
    xs[i + 1] = rk4(z, dt)
xs = jnp.hstack(xs)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x1, x2 = xs
fig, ax = plt.subplots(1, figsize=[8,8])
ax.plot(x1, x2, lw=5)
ax.set_aspect(&#39;equal&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_ff99742bcbe18cf709e955ffc73a47f4.png 400w,
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_32f801381eaeff81727c764fd973909e.png 760w,
               /posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_40_0_huf9ba5c23501be01829d4667c481b151b_17632_ff99742bcbe18cf709e955ffc73a47f4.png&#34;
               width=&#34;247&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Repeat the same differentiation process using JAX to get improved derivatives inherited directly from the improved numerical integration.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Differentiate
jac_rk4 = jacfwd(rk4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;jac_xs = [None] * len(ts)
det_jac_xs = [None] * len(ts)
for i, t in tqdm(enumerate(ts), total=len(ts)):
    z = jnp.vstack([xs[:, i][:, None], u_fn(t)])
    jac_xs[i] = jac_rk4(z.squeeze(), dt)
    det_jac_xs[i] = np.linalg.det(jac_xs[i][:, :2])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  0%|          | 0/100 [00:00&amp;lt;?, ?it/s]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Find the differential of F
dx = xs[:, 1:] - xs[:, :-1]
df = np.vstack([jac_xs[i][:,:2] @ dx[:, i] for i in range(len(ts))]).T
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Plot tangent vectors colored by the determinant of the Jacobian
fig, ax = plt.subplots(1, figsize=[8, 8])
cmap = plt.get_cmap(&#39;viridis&#39;)
norm = plt.Normalize(min(det_jac_xs), max(det_jac_xs))
for i in range(len(ts)):
    ax.arrow(x1[i], x2[i], df[0][i], df[1][i], width=.05,
             color=cmap(norm(det_jac_xs[i])))
ax.set_aspect(&#39;equal&#39;)
fig.colorbar(ScalarMappable(norm=norm, cmap=cmap))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_4c630a5bdc5bf5a8dbc06632e4f234b5.png 400w,
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_c398f8c18dba9a2ba70ca381359fe9ed.png 760w,
               /posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://andgoldschmidt.github.io/posts/jax_mpc/output_45_1_hu4f73d3c8882ffb8b9f49e9d2c6a9f7dd_32136_4c630a5bdc5bf5a8dbc06632e4f234b5.png&#34;
               width=&#34;318&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Nice derivatives.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Control basics with CartPole</title>
      <link>https://andgoldschmidt.github.io/posts/cartpole/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://andgoldschmidt.github.io/posts/cartpole/</guid>
      <description>&lt;p&gt;In this &lt;a href=&#34;https://github.com/andgoldschmidt/cartpole&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project on Github&lt;/a&gt; I coded up some tutorial concepts in control theory like observability, controllability, and the linear quadratic regulator using the example environment of a linear pendulum fixed to a cart. I also made fun Jupyter notebook movies to visualize the results.&lt;/p&gt;
&lt;p&gt;Also included under this project are the slides I did for a short class project covering reinforcement learning (RL) for CartPole from the OpenAI lab. It&amp;rsquo;s nice to contextualize model-free RL methods for control within a familiar environment where we have covered the control theory basics.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TODO: Turn this into a Google Colab&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://colab.research.google.com/&lt;/a&gt;
Embedding the notebook as a static webpage means there&amp;rsquo;s not a lot of fun to have with the movies so come back soon for a dynamic version of this post.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
