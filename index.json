[{"authors":null,"categories":null,"content":"Andy is a Physics PhD student working on quantum optimal control as an NSF QISE-NET fellow in collaboration with Lawrence Livermore National Laboratory. He also worked with the Pacific Northwest Research Institute on the inverse design of better commerical brewing and baking yeasts. Andy enjoys highly-collaborative environments and likes to seek out out ways to turn new scientific discoveries into practical applications using data-driven methods. Andy is advised by J. Nathan Kutz, Robert Bolles and Yasuko Endo Professor of Applied Math at the University of Washington.\nEven though he is attached to the Applied Math Department, Lawrence Livermore National Laboratory, and the Pacific Northwest Research Institute, Andy remains an active part of the University of Washington Department of Physics. He has organized two iteratons (2018, 2019) of a multi-day networking event for the Physics Graduate School with invited industry sponsors, speakers, and alumni guests. Andy is also an officer of the Union Bay Rowing Club at the University of Washington and plays for Jet City F.C. in the Greater Seattle Soccer League. Andy is pictured here with his dog, Susie.\nAndy is actively looking for quantum control and physics-informed machine learning Postdocs for Fall 2022!\n  Download my C.V.\n","date":1616371200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1616371200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Andy is a Physics PhD student working on quantum optimal control as an NSF QISE-NET fellow in collaboration with Lawrence Livermore National Laboratory. He also worked with the Pacific Northwest Research Institute on the inverse design of better commerical brewing and baking yeasts.","tags":null,"title":"Andy J. Goldschmidt","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://andgoldschmidt.github.io/slides/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"slides"},{"authors":[],"categories":null,"content":"","date":1632614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632614400,"objectID":"0d68c887cc23ebc354d1e3eebe82fc9d","permalink":"https://andgoldschmidt.github.io/talk/talk-bilinear-dynamic-mode-decomposition-for-quantum-control-at-mmldt-cset/","publishdate":"2021-09-26T00:00:00Z","relpermalink":"/talk/talk-bilinear-dynamic-mode-decomposition-for-quantum-control-at-mmldt-cset/","section":"event","summary":"In a single framework, biDMD accommodates unknown quantum control dynamics in an interpretable way","tags":[],"title":"(Talk) Bilinear Dynamic Mode Decomposition for Quantum Control at MMLDT-CSET","type":"event"},{"authors":["Andy J. Goldschmidt","E. Kaiser","J.L. DuBois","S.L. Brunton","J.N. Kutz"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.   --  Create your slides in Markdown - click the Slides button to check out the example.   -- Caption for the figure: The trajectory of a qubit driven by a linearly-polarized semi-classical drive $\\textrm{u}(t)$ (Hamiltonian: $H(t) = \\pi \\sigma_z + \\textrm{u}(t) \\sigma_x$) is shown on the Bloch sphere in (a). The corresponding Pauli-spin measurements are shown in (b). Measurements $\\mathbf{x}_j$, $j=1,2,\\dots$, are taken at discrete time steps and assembled into offset snapshot matrices $\\mathbf{X}$ and $\\mathbf{X}'$ in (c). The bilinear Dynamic Mode Decomposition (d) is a regression-based algorithm that uses the assembled data matrices and control input from sufficiently-resolved data to learn the intrinsic dynamics, $\\mathbf{A}$ and the control, $\\mathbf{B}$, for the bilinear dynamics.\n","date":1616371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616371200,"objectID":"883fe5b5c304c19f799e63d2fea508db","permalink":"https://andgoldschmidt.github.io/publication/bidmd/","publishdate":"2021-03-22T00:00:00Z","relpermalink":"/publication/bidmd/","section":"publication","summary":"We develop a data-driven regression procedure, _bilinear dynamic mode decomposition_ (biDMD), that leverages time-series measurements to establish quantum system identification for quantum optimal control.","tags":[],"title":"Bilinear dynamic mode decomposition for quantum control","type":"publication"},{"authors":[],"categories":null,"content":"Session 1     9:45-10:00 Calibrating Quantum Hardware with Online Optimal Control abstract   Jonathan L. Dubois, Lawrence Livermore National Laboratory, U.S.   10:05-10:20 Bilinear Dynamic Mode Decomposition for Quantum Control abstract   Andy Goldschmidt and Eurika Kaiser, University of Washington, U.S.; Jonathan L. Dubois, Lawrence Livermore National Laboratory, U.S.; Steve Brunton, University of Washington, U.S.; J. Nathan Kutz, University of Washington, Seattle, U.S.   10:25-10:40 Designing High-Fidelity Controls on Real Quantum Systems using\u0026nbsp;System\u0026nbsp;Identification and Reinforcement Learning abstract   Michael J. Biercuk, University of Sydney, Australia; Harrison Ball, Q-CTRL, Australia; Yuval Baum, California Institute of Technology, U.S.; Andre Carvalho, Griffith University, Brisbane, Australia; Leonardo de Castro, Sean Howell, Michael Hush, and Per Liebermann, Q-CTRL, Australia; Pranav Mundada, Princeton University, U.S.; Felix Thomsen, Q-CTRL, Australia   10:45-11:00 Quantum System Compression: A Hamiltonian Guided Walk through Hilbert Space abstract   Robert L. Kosut, SC Solutions, U.S.; Herschel Rabitz and Tak-San Ho, Princeton University, U.S.   11:05-11:20 Learning the States of Quantum Dot Systems: A Ray-Based Framework abstract   Justyna Zwolak, National Institute of Standards and Technology, U.S.   Session 2     2:15-2:30 A General Theory of Randomized Benchmarking abstract   Jens Eisert, Freie Universität Berlin, Germany; Jonas Helsen, University of Amsterdam, Netherlands; Ingo Roth, Freie Universität Berlin, Germany; Emilio Onorati, University College London, United Kingdom; Albert H. Werner, University of Copenhagen, Denmark   2:35-2:50 Operational, Gauge-Free Quantum Tomography abstract   Olivia Di Matteo, TRIUMF, Canada; John Gamble and Chris Granade, Microsoft Research, U.S.; Kenneth Rudinger, Sandia National Laboratories, U.S.; Nathan O. Wiebe, University of Washington, U.S.   2:55-3:10 Linear Preservers on Infinitely Divisible Matrices through Separability abstract   Indu L and Jill K Mathew, Mar Ivanios College, Trivandrum, Kerala, India   3:15-3:30 Machine Learning for Quantum States abstract   Giuseppe Carleo, EPFL, Switzerland   3:35-3:50 Learning Models of Quantum Systems from Experiments abstract  Antonio Gentile and Brian Flynn, University of Bristol, United Kingdom; Sebastian Knauer, University of Vienna, Austria;  Nathan O. Wiebe, University of Washington, U.S.; Stefano Paesani, University of Bristol, United Kingdom; Chris Granade, Microsoft Research, U.S.; John Rarity, Raffaele Santagati, and Anthony Laing, University of Bristol, United Kingdom   ","date":1614600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614600000,"objectID":"db1189275b5c9472a85bab64fb200d8c","permalink":"https://andgoldschmidt.github.io/talk/organizer-data-driven-methods-for-quantum-dynamics-and-control-at-siam-cse/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/talk/organizer-data-driven-methods-for-quantum-dynamics-and-control-at-siam-cse/","section":"event","summary":"Minisymposium","tags":[],"title":"(Organizer) Data-Driven Methods for Quantum Dynamics and Control at SIAM CSE","type":"event"},{"authors":null,"categories":null,"content":"Helper functions for spectral DMD. # MIT License # Copyright (c) 2021 Andy Goldschmidt # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \u0026quot;Software\u0026quot;), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in all # copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \u0026quot;AS IS\u0026quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE # SOFTWARE. import numpy as np def BigOmg(omega, ts): ''' Construct time-domain coordinates for Spectral DMD. Args: omega (`1d-array`): 1d array of frequencies ts (`1d-arary`): 1d array of times Returns: `nd-array`: 2*len(omega) by len(t) array of $[\\cos(\\vec{omega} t), \\sin(\\vec{omega} t)]^T$ ''' omt = omega.reshape(-1,1)@ts.reshape(1,-1) return np.vstack([np.cos(2*np.pi*omt), np.sin(2*np.pi*omt)]) def loss(X, A, omega, ts): ''' Loss function for Spectral DMD. Returns: float: Evaluation of loss function. ''' return np.linalg.norm(X - A@BigOmg(omega,ts)) def grad_loss(X, A, omega, ts): ''' Gradient of the loss function w.r.t. $\\omega$. ''' n_omg = len(omega) part2 = -4*np.pi*A.T@(X - A@BigOmg(omega,ts)) grad_res = [0]*n_omg for i in range(n_omg): grad_res[i] += (-np.sin(2*np.pi*omega[i]*ts)*ts).dot(part2[i,:]) grad_res[i] += (np.cos(2*np.pi*omega[i]*ts)*ts).dot(part2[n_omg + i,:]) return np.array(grad_res).reshape(1,-1) def grad_loss_j(j, X, A, omega, ts): ''' Gradient of the loss function w.r.t. $\\omega_j$ Returns: float: Gradient of the loss w.r.t $\\omega_j$ ''' n_omg = len(omega) if j \u0026gt; n_omg - 1: raise ValueError(\u0026quot;Invalid value. Index j={} exceeds len(omega)={}.\u0026quot;.format(j,n_omg)) part2 = -4*np.pi*A.T@(X - A@BigOmg(omega,ts)) grad_res_j = (-np.sin(2*np.pi*omega[j]*ts)*ts).dot(part2[j,:]) grad_res_j += (np.cos(2*np.pi*omega[j]*ts)*ts).dot(part2[n_omg + j,:]) return grad_res_j def residual_j(j, X, A, omega, ts): ''' Residual for the data trajectory X using the model A, $\\Omega$. The residual excludes the contribution of the frequency $\\omega_j$. TODO: * Check the case where A.shape[0] = 1 Returns: `ndarray`: residual of shape X.shape[0] by ts.shape[0] ''' n_omega = len(omega) if j \u0026gt; n_omega - 1: raise ValueError(\u0026quot;Invalid value. Index j={} exceeds len(omega)={}.\u0026quot;.format(j, n_omega)) j2 = j + n_omega indices = np.hstack([np.arange(j), np.arange(j+1,j2), np.arange(j2+1, 2*n_omega)]) return X - A[:,indices]@(BigOmg(omega,ts)[indices, :]) def update_A(X, omega, ts, threshold, threshold_type): # Update A # -- This step could get some DMD love U,S,Vt = np.linalg.svd(BigOmg(omega, ts), full_matrices=False) if threshold_type == 'count': r = threshold elif threshold_type == 'percent': r = np.sum(S/np.max(S) \u0026gt; threshold) rU = U[:,:r] rS = S[:r] rVt = Vt[:r, :] return X@np.conj(rVt.T)@np.diag(1/rS)@np.conj(rU.T) def max_fft_update(result, dt): # Real signal means the other half of the hat are complex conjugates n = result.shape[1] n_sym = n//2 if n % 2 == 0 else n//2 + 1 # Compute fft res_hat = np.fft.fft(result, axis=1) # Get the maximum freq. coordinate considering all data dimensions ires = np.argmax(np.sum(np.abs(res_hat[:,:n_sym]), axis=0)) res_freq = np.fft.fftfreq(n, dt)[:n_sym] return res_freq[ires] # Accelerated proximal gradient descent # ----------------------------------------------------------------------------- def optimizeWithAPGD(x0, func_f, func_g, grad_f, prox_g, beta_f, tol=1e-6, max_iter=1000, verbose=False): \u0026quot;\u0026quot;\u0026quot; Optimize with Accelerated Proximal Gradient Descent Method min_x f(x) + g(x) where f is beta smooth and g is proxiable. input ----- x0 : array_like Starting point for the solver func_f : function Input x and return the function value of f func_g : function Input x and return the function value of g grad_f : function Input x and return the gradient of f prox_g : function Input x and a constant float number and return the prox solution beta_f : float beta smoothness constant for f tol : float, optional Gradient tolerance for terminating the solver. max_iter : int, optional Maximum number of iteration for terminating the solver. output ------ x : array_like Final solution obj_his : array_like Objective function value convergence history err_his : array_like Norm of gradient convergence history exit_flag : int 0, norm of gradient below `tol` 1, exceed maximum number of iteration 2, others \u0026quot;\u0026quot;\u0026quot; # initial information x = x0.copy() y = x0.copy() g = grad_f(y) t = 1.0 # step_size = 1.0/beta_f # not recording the initial point since we do not have measure of the optimality obj_his = np.zeros(max_iter) err_his = np.zeros(max_iter) # start iteration iter_count = 0 err = tol + 1.0 while err \u0026gt;= tol: ##### # Accelerated proximal gradient x_new = prox_g(y - step_size*g, step_size) t_new = (iter_count - 1)/(iter_count + 2) y_new = x_new + t_new*(x_new - x) # FISTA version: # t_new = (1 + np.sqrt(1+4*t**2))/2 # y_new = x_new + (t - 1)/t_new*(x_new - x) ##### # # update information obj = func_f(x_new) + func_g(x_new) err = np.linalg.norm(x - x_new) # np.copyto(x, x_new) np.copyto(y, y_new) t = t_new g = grad_f(y) # obj_his[iter_count] = obj err_his[iter_count] = err # # check if exceed maximum number of iteration iter_count += 1 if iter_count \u0026gt;= max_iter: if verbose: print('Proximal gradient descent reach maximum of iteration') return x, obj_his[:iter_count], err_his[:iter_count], 1 # return x, obj_his[:iter_count], err_his[:iter_count], 0  ","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"98d557db8f06ea677ae37719a2998d28","permalink":"https://andgoldschmidt.github.io/pages/spectral_help/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/pages/spectral_help/","section":"pages","summary":"Helper functions for spectral DMD. # MIT License # Copyright (c) 2021 Andy Goldschmidt # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the \u0026quot;Software\u0026quot;), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in all # copies or substantial portions of the Software.","tags":null,"title":"","type":"pages"},{"authors":null,"categories":null,"content":"Spectral dynamic mode decomposition is my term for Algorithm 1 from the paper From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction by H. Lange, S.L. Brunton, J.N. Kutz (video) (arXiv). Necessary background is a familiarity with the dynamic mode decomposition (video) (Wikipedia entry).\nThis project walks through a toy example I coded up in Python. The example is similar to one from the paper which gets across the main points. I have packed away the main functions for the algorithm in the utility spectral_help.py which has been linked in this project\u0026rsquo;s description.\nimport numpy as np from numpy.linalg import norm, solve import matplotlib.pyplot as plt cmap = plt.get_cmap('tab20') from spectral_help import *  Example Definition class toy_data: \u0026quot;\u0026quot;\u0026quot; Generate toy oscillation data at specific frequencies. \u0026quot;\u0026quot;\u0026quot; def __init__(self, tf, npts, noise): \u0026quot;\u0026quot;\u0026quot; Parameters: tf: final time npts: number of samples between 0 and tf noise: standard deviation of additive Gaussian noise \u0026quot;\u0026quot;\u0026quot; self.tf = tf self.npts = npts self.ts = np.linspace(0, self.tf, self.npts) self.dt = self.ts[1] - self.ts[0] self.noise = noise # Manual toy data (a stack of sines) self.freqs = [0.5, 2, 0.75, 3] self.X_fn = lambda ts: np.vstack([(np.sin([2 * np.pi * self.freqs[0] * ts]) + np.sin([2 * np.pi * self.freqs[1] * ts])), (np.sin([2 * np.pi * self.freqs[2] * ts]) + np.sin([2 * np.pi * self.freqs[3] * ts]))]) # Normalize, add noise X = self.X_fn(self.ts) self.X_mean = np.mean(X, axis=1).reshape(-1,1) self.X_std = np.std(X, axis=1).reshape(-1,1) X = (X - self.X_mean)/self.X_std self.X_true = np.copy(X) self.X = X + np.random.randn(*X.shape)*self.noise # Construct test vars self.ts_test = None self.X_test = None def run_test(self, tf_predict, npts_predict): \u0026quot;\u0026quot;\u0026quot; Parameters: tf_predict: final time npts_predict: number of points between 0 and tf_predict Updates: self.ts_test: test time series self.X_test_true: normalized ground truth for the test \u0026quot;\u0026quot;\u0026quot; self.ts_test = np.linspace(0, tf_predict, npts_predict) X_test = self.X_fn(self.ts_test) # Use training std_dev and mean self.X_test_true = (X_test - self.X_mean)/self.X_std  Run the Experiment We run the spectral dynamic mode decomposition algorithm to learn the frequencies of an operator generating our toy time-series data. Because of the nature of the algorithm, we can do this with very noisy data (Here, we set the standard deviation at 0.5 for mean-zero variance-one toy training data).\nFirst, we configure the model. Then we set up the algorithm and run the optimization. The optimization is performed using the Accelerated Proximal Gradient Descent Method, or AGPD. The reason is because we are imposing sparsity using a $\\ell_1$ norm\u0026ndash;that is, a LASSO-type optimization.\n1. Configure the model # Config # ====== np.random.seed(1) # Data params npts = 400 # number of time points tf = 8 # max time std_noise = .5 # set the amount of noise on the mean-0, variance-1 data predict_factor = 3 # prediction goes out to factor*T exper = toy_data(tf, npts, std_noise) # Algorithm params freq_dim = 24 # freq. for algo to try learning_rate = 1e-3 # LR = 1/beta from beta-smooth obj. bound (at least ideally--I'm just choosing a number here) reg_factor = 5 # regularization on sparsity # SVD parameters threshold_type = 'percent' # choose 'count' for number or 'percent' for s/max(s) \u0026gt; threshold threshold = 1e-1 # Plot toggle print_omega_updates = True def print_update(omg, title): if print_omega_updates: print('{} $\\omega$:\\t'.format(title), np.sort(np.round(omg[omg.astype(bool)], 3)))  2. Set up the algorithm \u0026amp; 3. Optimize # Algorithm # ========= # 1. Initialize omega = np.zeros(freq_dim)*2 print_update(omega, 'Initial') A = np.random.rand(exper.X.shape[0], freq_dim*2) obj_his = [] err_his = [] # 2. FFT to obtain the initial starting point for the optimization for ifreq in range(len(omega)): # - Construct the residual via the current frequencies res = residual_j(ifreq, exper.X, A, omega, exper.ts) # - Select the maximum fft frequency as the initial value omega[ifreq] = max_fft_update(res, exper.dt) # - Update A A = update_A(exper.X, omega, exper.ts, threshold, threshold_type) # 3. Perform proximal gradient descent from the initial point # - Construct optimization functions lam_cs = reg_factor*norm(A.T.dot(exper.X), np.inf) def f(w): return loss(exper.X, A, w.flatten(), exper.ts) def gradf(w): return grad_loss(exper.X, A, w.flatten(), exper.ts) def func_g(w): return lam_cs*np.linalg.norm(w, ord=1) def prox_g(w, t): res = [] r = t*lam_cs for wi in w.flatten(): if wi \u0026gt; r: res.append(wi-r) elif wi \u0026lt; -r: res.append(wi+r) else: res.append(0) return np.array(res) # - Optimization algorithm w, iobj_his, ierr_his, cond = optimizeWithAPGD(omega.reshape(1,-1), f, func_g, gradf, prox_g, (1/learning_rate)*npts, max_iter=5000, verbose=True) obj_his.append(iobj_his) err_his.append(ierr_his) omega = w print_update(omega, 'Final ') # 4. Final operator update A = update_A(exper.X, omega, exper.ts, threshold, threshold_type) print_update(np.array(exper.freqs), 'Expected')  Initial $\\omega$:\t[] Final $\\omega$:\t[0.494 0.743 1.991 2.99 ] Expected $\\omega$:\t[0.5 0.75 2. 3. ]  We printed out the frequencies (previous cell). In the next cell, we show the objective value and the gradient of the objective for the iterations of the optimization. The characteristic oscillations of an accelerated gradient descent are observed.\n# Plot # ==== # Inspect convergence results fig,axes = plt.subplots(2,1,figsize=[10,10]) ax = axes[0] ax.plot(iobj_his) ax.set_ylabel('Obj. value') ax.set_xlabel('Iterations') ax.set_yscale('log') ax = axes[1] ax.plot(ierr_his) ax.set_ylabel('Gradient magn.') ax.set_xlabel('Iterations') ax.set_yscale('log')     Result We see the training (top) and test (bottom) results for our two-dimensional multi-frequency dynamics.\nNotice the order of magnitude increase in the horizontal time axis on the bottom plot. On this plot, we observe that the solutions match the test simulations and are stable because of the model assumptions.\n# Make prediction exper.run_test(tf*predict_factor, 10*npts) # keep sample freq the same bigOmg_test = BigOmg(omega, exper.ts_test) X_pred = A@(bigOmg_test) # Plot data fig,axes = plt.subplots(1,2,figsize=[20,3]) fig.subplots_adjust(hspace=0.3, wspace=0.2) leg_params = {'loc': 'upper right', 'shadow': True, 'fancybox': True} ax = axes[0] ax.plot(exper.ts, exper.X_true[0], color=cmap(1), label='Simulation') ax.plot(exper.ts, exper.X[0], ls='', marker='+', color=cmap(0), label='Training Data') ax.set_xlabel('t') ax.set_ylabel('x') ax.legend(**leg_params) ax = axes[1] ax.plot(exper.ts, exper.X_true[1], color=cmap(3), label='Simulation') ax.plot(exper.ts, exper.X[1], ls='', marker='+', color=cmap(2), label='Training Data') ax.legend(**leg_params) ax.set_xlabel('t') ax.set_ylabel('y') ax.set_ylim([-3.5,3.5]) # Plot model fig,axes = plt.subplots(2,1,figsize=[20,5]) fig.subplots_adjust(hspace=0.3, wspace=0.2) leg_params = {'loc': 'upper right', 'shadow': True, 'fancybox': True} ax = axes[0] ax.plot(exper.ts_test, exper.X_test_true[0], color=cmap(1), label='Test Simulation') ax.plot(exper.ts_test, X_pred[0], ls='-', color=cmap(0), label='Model Prediction') ax.legend(**leg_params) ax.set_xlabel('t') ax.set_ylabel('x') ax = axes[1] ax.plot(exper.ts_test, exper.X_test_true[1], color=cmap(3), label='Test Simulation') ax.plot(exper.ts_test, X_pred[1], ls='-', color=cmap(2), label='Model Prediction') ax.legend(**leg_params) ax.set_xlabel('t') ax.set_ylabel('y') ax.set_ylim([-3.5,3.5])  (Click on the figures to zoom.)\nTraining data:   Test data:   ","date":1609718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609718400,"objectID":"bfa176bed1fcf2c0c60512810faa4bb0","permalink":"https://andgoldschmidt.github.io/project/spectral_dmd/","publishdate":"2021-01-04T00:00:00Z","relpermalink":"/project/spectral_dmd/","section":"project","summary":"Exploring a method for long time-series forecasting.","tags":["Machine Learning"],"title":"Spectral dynamic mode decomposition","type":"project"},{"authors":["Andy J. Goldschmidt","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://andgoldschmidt.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"Derivative is an open-source project I started in 2019-2020 that turned into a collaboration with Markus Quade (Github, @Ohjeah) and Brian de Silva (Github, @briandesilva). It is a standalone suite of numerical differentiation methods for noisy time series data written in Python.\nThe goal is to provide some common numerical differentiation techniques that showcase improvements that can be made on finite differences when data is noisy. The package binds these common differentiation methods to a single easily implemented differentiation interface to encourage user adaptation.\nDerivative is a contribution to PySINDy\n           PySINDy is an open source Python package for the Sparse Identification of Nonlinear Dynamical systems (SINDy).\nAt some point, I\u0026rsquo;ll write a post about my version of total variational regularization (see the figure above). I adapted a technique from The solution path of the generalized lasso (DOI: 10.1214/11-AOS878) by R.J. Tibshirani \u0026amp; J. Taylor to write a nice variation of the classic algorithm in Numerical Differentiation of Noisy, Nonsmooth Data (DOI: 10.5402/2011/164564) by Rick Chartrand.\n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"8ba44b6b7e78d6b1eb2a696c2585e941","permalink":"https://andgoldschmidt.github.io/project/derivative/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/project/derivative/","section":"project","summary":"Open-source Python code for numerical differentiation of noisy time series data.","tags":["Machine Learning"],"title":"Derivative","type":"project"},{"authors":null,"categories":null,"content":"Pyprotoclust is an implementatin of representative hierarchical clustering using minimax linkage. The original algorithm is from Hierarchical Clustering With Prototypes via Minimax Linkage (DOI: 10.1198/jasa.2011.tm10183) by J. Bien and R. Tibshirani; Pyprotoclust takes a distance matrix as input. It returns a linkage matrix encoding the hierachical clustering as well as an additional list labelling the prototypes associated with each clustering.\nI coded up a fun example inspired by the original paper where I apply the algorithm to determine representative pictures for the Olivetti Faces dataset. It can be found in the Pyprotoclust documentation.\nFigure: (Left) A dendrogram of the hierarchical clustering example with a dashed line at the example cut height. (Right) A scatter plot of the example with circles centered at prototypes drawn with radii equal to the top-level linkage heights of each cluster.\n","date":1595721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595721600,"objectID":"00e7c4e3703c6d4de73dd2a0a73856d7","permalink":"https://andgoldschmidt.github.io/project/pyprotoclust/","publishdate":"2020-07-26T00:00:00Z","relpermalink":"/project/pyprotoclust/","section":"project","summary":"An open-source Cython implementation of representative hierarchical clustering using minimax linkage.","tags":["Machine Learning"],"title":"Hierarchical Clustering with Prototypes","type":"project"},{"authors":null,"categories":null,"content":"There are lots of features that go into packaging up Python code for other users. How can we make a project that ships easily to users and takes advantage of our normal development tools? We\u0026rsquo;ll discuss:\n Poetry: for easily making and publishing a package Sphinx: for making documentation Readthedocs: free professional-looking documenation hosting and formatting PyCharm: the default Python IDE (you can get the professional version as a student) PyPI: Python pacakge index, where you store stuff that people can pip install.  Basics Read the Poetry docs to install. It\u0026rsquo;s good documentation; you should skim the Installation and Basic Usage first. For zsh users, make sure Poetry got added to your Path inside zshrc. For bash people, it\u0026rsquo;s automatic.\nLet\u0026rsquo;s make a new project with Poetry.\n Create a python project. (See Choosing a project name below this list.) poetry new myproject  Change to this directory.\n Start the poetry virtual environment. poetry shell  This will make a virtual environment that is like a fresh Python installation for us to be explicit about our package\u0026rsquo;s dependencies.\n Install new pacakges as needed. poetry add python_package  The name python_pacakge would be something like numpy. Poetry will install the package to the virtual environment and add the package to the pyproject.toml file.\n Take a look at the pyproject.toml file. All your package settings are here. Any added pacakges appear automatically. We also have a file not for human consumption called poetry.lock which does all the work of building the exact environement we are using. It can be good to commit this poetry.lock to version control so someone interacting with our package knows the exact packages we were using. To update all pacakges added with poetry add, run poetry update. To install the current system and update the poetry.lock file, run poetry install.  Choosing a project name A new poetry project ``myproject'' has a specific directory structure:\n myproject |-- pyproject.toml |-- README.rst |-- myproject | |-- __init__.py |-- tests |-- __init__.py |-- test_myproject.py  The project name is for the top level directory of the project. This is the name of the github repository and the PyPI project so it is invoked with pip install myprojectname. It should be unique. Project names on PyPI should NOT use dashes (https://stackoverflow.com/questions/8350853/). Underscores are allowed but discouraged. The package or module name is the inner directory containing __init__.py. This is the code that will be invoked by the user as import my_project_name in code. It does not have to be unique. It can use underscores.Note that Poetry defaults to matching project and package names. This is also the Python style guideline (PEP 423).\nVersion control Now we start tracking our new package on version control. Init a git repo in the project directory. Do this in your usual way. (e.g. hosting on github). The splash page for your package will be README.rst! Make it pretty.\nPyCharm Let\u0026rsquo;s use a modern IDE. Open up the project in PyCharm.\n  Get the location of the interpreter for this virtual environment for PyCharm. Run this command:\npoetry run which python    Make the virtual environment default for PyCharm.\n Settings $\\rightarrow$ Project $\\rightarrow$ Python Interpreter Click the gear, and select add. Choose the option \\textit{existing environment} and add the path to the poetry virtual environment. Apply changes.  Now PyCharm will complain when you try to use python code you haven\u0026rsquo;t added. PyCharm will also give you actions to import missing libraries. However, be sure to add the python libraries with poetry, not PyCharm.\n  Add documentation with Sphinx Let\u0026rsquo;s set up Sphinx.\n  Start\nmkdir docs poetry add sphinx cd docs sphinx-quickstart  The command line will prompt you with a few questions. Use the default settings, but enter any project-specific information as needed.\n  All Sphinx settings are in conf.py. The first setting to edit is the path. Uncomment the lines:\nimport os import sys sys.path.insert(0, os.path.abspath('.'))  and change the \u0026ldquo;.\u0026rdquo; to \u0026ldquo;..\u0026rdquo; to reflect the docs folder.\n  Make sure that Sphinx knows that the main file is index.rst by adding the lines\n# Assign the master document master_doc = 'index'  to conf.py.\n  Test to see that your docs compile. Run the command\nmake html  inside the docs folder then open up index.html in your web browser.\n  Add docs to readthedocs.\n Go to readthedocs, login, then find and click import. Paste the link to the github repo and create.  Readthedocs will find the conf.py file and build the documentation.\nCheck that github will let readthedocs know when the documentation is updated. Go to the project repo settings and confirm that the Webhooks tab includes readthedocs.    Note: Oddly, the default Poetry config section tool.poetry.dependencies that allows users of your package to avoid installing development tools like Sphinx is not supported by readthedocs (the listed packages won\u0026rsquo;t be used). There is an alternative, e.g.\nsphinx = {version=\u0026quot;^3.0.2\u0026quot;, optional = true}  To add packages to Poetry as optional you can call poetry add sphinx \u0026ndash;optional to autofill this format. In your .readthedocs.yaml file, you can make sure these packages are installed by adding the extra_name parameter to extra_requirements, e.g.:\npython: version: 3.7 install: - method: pip path: . extra_requirements: - docs  Publish the package on PyPI This is as easy as poetry publish! First, we\u0026rsquo;ll have to setup our PyPI whic we can do by following the Poetry documentation.\nAdditional tasks Sphinx can automatically generate documentation for the modules, classes, and functions that have properly formatted docstrings. There are two main docstring styles: NumPy and Google. I use Google\u0026rsquo;s docstring format becaues it takes up less vertical space. The essential Sphinx extensions are autodoc (for automatically making docstrings into reStructuredText) and napoleon (for docstring formats). Both should be added to the Sphinx conf.py file as extensions,\nextensions = ['sphinx.ext.autodoc','sphinx.ext.napoleon']  No installation by Poetry is necessary because both are part of the base installation of Sphinx.\nA common point of troubleshooting is that the readthedocs servers do not have your desired library installed. You will need to go to Advanced Settings on readthedocs and make sure you select to use both\nInstall Project Install your project inside a venv using setup.py install Use system packages Give the venv access to the global site packages dir  Technically, you may only need the second option to get e.g. numpy which readthedocs has installed on their servers for you. But if you want a more advanced option like sklearn that isn\u0026rsquo;t on the default servers, you\u0026rsquo;ll need to install the project. This means you need one more file at the top level of your project called .readthedocs.yaml which looks something like\nversion: 2 build: image: latest python: version: 3.7 install: - method: pip path: . extra_requirements: - docs sphinx: configuration: docs/conf.py  This file makes sure that the setup.py command (the old package tool) interacts with the poetry configuration files correctly. Currently, there are some changes to python standards moving in poetry\u0026rsquo;s direction, but these are not implemented in readthedocs yet. Hence, this extra file.\nAdding Jupyter notebooks to the docs The key tool here is nbsphinx. This will need to be installed by poetry. Also, an ipython kernel and a jupyter reader will need to be installed for readthedocs to run the notebook (explicitely, poetry add ipykernel and poetry add jupyter_client. You can make these optional. You will also need the .readthedocs.yaml file so see the note at the end of the autodoc section.\nAdding a LICENSE  Create a file in docs called license.rst and give the file a header like  License ======= ...  Inside the index.rst look for  .. toctree:: :maxdepth: 2 :caption: Contents: license  where we have added license to link the license file to the main documentation page (the name of the link will reflect the headings/subheadings in the file license.rst).\nCython Development I think the most effective cython tutorial is this cython documentation example. You\u0026rsquo;ll eventually be introduced to the very basic example:\nfrom setuptools import setup from Cython.Build import cythonize setup(ext_modules=cythonize(\u0026quot;rect.pyx\u0026quot;))  This example is good because it shows the essential features. However, if you have multiple c++ files that you want to compile together, you will need more. Eventually you\u0026rsquo;ll want to create more complicated objects to pass to cythonize/ext_modules. This will introduce you to distutils.\nThe main change to have cython code is to add a build.py file. This file uses the python library distutils to link all the c++ files and call cython. I have an example build.py on my Github. To get Poetry to use your build.py, you need to include {build = 'build.py'} under the [tool.poetry] section in your pyproject.toml.\nFor readthedocs, it seems that a wrapper around this build.py script is needed. A short setup.py script can be written to do this. You might want to look at additional tasks for some context on parts of this, but here\u0026rsquo;s an example:\n# Wrapper over build.py for readthedocs from distutils.core import setup from build import build global setup_kwargs setup_kwargs = {} build(setup_kwargs) setup(**setup_kwargs)  Another issue to address with readthedocs is making sure autodoc works for cython code. This fix is courtesy of https://stackoverflow.com/questions/13238736.\nUsing Docker The goal of this section is to build and distribute a C++ shared package called example_package inside a Python wheel using poetry and auditwheel. For this, I followed: https://github.com/riddell-stan/poetry-install-shared-lib-demo.\nThe wheel created using these instructions conforms to the manylinux2014 standard and should be usable on most Linux systems. This README also includes notes which may be of interest to developers seeking to understand how the auditwheel repair command works. You\u0026rsquo;ll need to install docker (so we can use PyPA\u0026rsquo;s manylinux2014 build image).\n","date":1589587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589587200,"objectID":"99816459ad35790ef7b9af50d3d70836","permalink":"https://andgoldschmidt.github.io/project/poetry/","publishdate":"2020-05-16T00:00:00Z","relpermalink":"/project/poetry/","section":"project","summary":"A post on how to make Python packages with Poetry.","tags":["Posts"],"title":"Python package development","type":"project"},{"authors":null,"categories":null,"content":"In this project on Github I coded up some tutorial concepts in control theory like observability, controllability, and the linear quadratic regulator using the example environment of a linear pendulum fixed to a cart. I also made fun Jupyter notebook movies to visualize the results.\nAlso included under this project are the slides I did for a short class project covering reinforcement learning (RL) for CartPole from the OpenAI lab. It\u0026rsquo;s nice to contextualize model-free RL methods for control within a familiar environment where we have covered the control theory basics.\nTODO: Turn this into a Google Colab https://colab.research.google.com/\n","date":1575849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575849600,"objectID":"3800916337de56838f992438371e4de4","permalink":"https://andgoldschmidt.github.io/project/cartpole/","publishdate":"2019-12-09T00:00:00Z","relpermalink":"/project/cartpole/","section":"project","summary":"Control theory basics using a rigid pendulum fixed to a cart.","tags":["Control","Machine learning"],"title":"CartPole","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://andgoldschmidt.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]